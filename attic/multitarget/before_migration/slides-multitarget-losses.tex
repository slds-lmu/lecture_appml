\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-multitarget}

\usepackage{multicol}
\usepackage{color,colortbl} 
\definecolor{putblue}{RGB}{0,0,124}
\definecolor{putred}{RGB}{204,33,69}

\newcommand{\titlefigure}{figure/fmeasure}
\newcommand{\learninggoals}{
    \item Get to know loss functions for multi-target prediction problems
    \item Understand the difference between instance-wise and decomposable losses 
    \item Know risk minimizer for Hamming and subset $0/1$ loss
}

\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{Multi-Target Prediction: Loss Functions}
\lecture{Advanced Machine Learning}



\sloppy


\begin{frame}{Multivariate Loss Functions}
	\begin{itemize}
		\item In MTP: For a feature vector $\xv$, predict a tuple of scores $f(\xv) = (f(x)_1, f(x)_2, \ldots, f(x)_l)^\top$ for $l$ targets with a function (hypothesis) $f: \Xspace \rightarrow \R^{g_1} \times \cdots \times \R^{g_l} $.
        
		\item Following loss minimization in machine learning, we need a \emph{multivariate loss function} 
		$$
		L: \, (\Yspace_1 \times \cdots \times \Yspace_l) \times (\R^{g_1} \times \cdots \times \R^{g_l}) \rightarrow \mathbb{R}.
		$$ 

        \item In multi-target regression: $\Yspace_1 = \ldots = \Yspace_l = \R$, and $g_1 = \ldots = g_l = 1$.

        \item In multi-label binary classification: $\Yspace_1 =  \ldots = \Yspace_l = \{0, 1\}$, and $g_1 = \ldots = g_l = 1$. %\\ $\leadsto$ I.e., each task is a binary classification.
	\end{itemize}
	
\end{frame}

\begin{frame}{Multivariate loss functions}
	\begin{itemize}
		\begin{minipage}{0.45\textwidth}	
			\item We treat two categories: Decomposable and instance-wise
            % \begin{itemize}
            %     \item Decomposable.
            %     \item Instance-wise.
            % \end{itemize}
		\end{minipage}
		\begin{minipage}{0.45\textwidth}
			\begin{center}
				\includegraphics[width=0.75\textwidth]{figure/fmeasure}
			\end{center}
		\end{minipage}
		
		\item $L$ is decomposable over targets if
		$$
		L(\yv, f) = \frac{1}{l} \sum_{m=1}^l L_m(y_m, f(\xv)_m) 
		$$
		with single-target losses $L_m$.     
     
		\item Example: \emph{Squared error loss} (in multivariate regression):
		$$
		L_{\text{MSE}}(\yv, f) = \frac{1}{l}\sum_{m=1}^l (y_m - f(\xv)_m)^2.
		$$
        \item Can also be used for cases with missing entries.
		
	\end{itemize}
\end{frame}

\begin{frame}{Instance-wise Losses}
	\begin{itemize}
		
        %\item Given a label vector $\yv$, it computes the loss on single instance. 

		\item \emph{Hamming loss} averages over mistakes in single targets:    
		$$
		\displaystyle L_H(\yv, \mathbf{h}) = \frac{1}{l}  \, \sum_{m=1}^l \, \mathds{1}_{[y_m \neq h_m(\xv)]},
		$$
        where $h_m(\xv) := [f(\xv)_m \geq c_m]$ is the threshold function for target $m$ with threshold $c_m$. 
        
        \item Hamming loss is identical to the average \emph{0/1 loss} %if computed on the entire dataset 
        and is decomposable.

		\item The \emph{subset 0/1 loss} checks for entire correctness and is not decomposable:  
		\begin{align*}
			\displaystyle L_{0/1}(\yv, \mathbf{h}) & = \mathds{1}_{[ \yv \neq \mathbf{h} ]}  = \max_m \, \mathds{1}_{[y_m \neq  h_m(\xv)]}
		\end{align*}
		
	\end{itemize}
\end{frame}


\begin{frame}{Hamming vs.\ subset 0/1 loss}
	\begin{itemize}
		\item The risk minimizer for the Hamming loss is the  \emph{marginal mode}:
		$$
		f^*(\xv)_m = \arg \max_{y_m \in \{0,1\}} \Pr( y_m  ~|~ \xv )\,, \quad m = 1,\ldots,l,
		$$
		%\item 
		while for the subset 0/1 loss it is the \emph{joint mode}:
		$$
		f^*(\xv) = \arg \max_{\yv} \Pr(\yv ~|~ \xv) \,.
		$$
		\item Marginal mode vs. joint mode:\\[6pt]
		\begin{center}
			\begin{tabular}{@{}cc@{}}
				\toprule
				$\yv$ & $\Pr(\yv)$ \\
				\hline
				$0~0~0~0$ & $0.30$ \\
				$0~1~1~1$ & $0.17$ \\
				$1~0~1~1$ & $0.18$ \\
				$1~1~0~1$ & $0.17$ \\
				$1~1~1~0$ & $0.18$ \\
				\toprule
			\end{tabular}
			$\qquad$
			\footnotesize{
				\begin{tabular}{lr}
					Marginal mode: & $1~1~1~1$ \\
					Joint mode: & $0~0~0~0$ \\
				\end{tabular}
			}
		\end{center}
	\end{itemize}
\end{frame}



%\begin{frame}
%	\frametitle{The individual target view}
%	
%	\begin{itemize} 
%		\item How can we improve the predictive accuracy of a single label \yv exploiting information about other labels?
%		\item Goal: predict a value of $y_i$ using $\xv$ and any available information on other targets $y_j$.
%		\item The problem is usually defined through univariate losses $\ell_i(y_i, \hat{y}_i)$.
%		%\item The problem is usually decomposable over the targets.
%		\item  Domain of $y_i$ is either continuous or nominal.
%		\item  Independent models vs.\ regularized (shrunken) models.
%%		\item James-Stein paradox (to be discussed later).
%		
%	\end{itemize}
%	
%\end{frame}
%
%
%
%\begin{frame}
%	\frametitle{The joint target view}
%	
%	\begin{itemize}
%		\item The problem is defined through multivariate losses $\ell(\yv, \yh)$.
%		%\item What are examples of such loss functions ?
%		\item Is reduction to single-target prediction (decomposition over targets) still possible, and even if so, can we improve over such strategies \yv using more expressive models?
%		%\item What are the relations between the losses?
%		%\item Goal: predict a vector $\yv$ using $\xv$.
%		\item Important: \emph{Structure of loss} $\ell(\cdot, \cdot)$, possible \emph{dependencies between targets}, multivariate distribution of $\yv$.
%		
%		%\item The problem might not be easily decomposable over the targets.
%		%\item Domain of $\yv$ is usually finite, but contains a large number of elements.
%		%\item Independent models vs.\ more expressive models.
%		%\item Loss function perspective.
%		
%	\end{itemize}
%\end{frame}



%
\endlecture
\end{document}