\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

%\usepackage{algorithm}
%\usepackage{algorithmic}

\usepackage{multicol}

\newcommand{\titlefigure}{figure/cost_matrix}
\newcommand{\learninggoals}{
  \item Cost matrix
  \item Minimum expected cost principle
  \item Optimal theoretical threshold
}

\title{Advanced Machine Learning}
\date{}

\begin{document}

\lecturechapter{Imbalanced Learning:\\
Cost-Sensitive Learning Part 1}
\lecture{Advanced Machine Learning}



\sloppy


\begin{vbframe}{Cost-Sensitive learning: In a Nutshell}
	%	
	\scriptsize{

		\begin{itemize}
		
    		\item Cost-sensitive learning: 
                \begin{itemize}
                    \scriptsize
                    \item Classical learning: data sets are balanced, and all errors have equal costs
                    \item We now assume given, unequal cost
                    \item And try to minimize them in expectation

                \end{itemize}
    		
    		\item Applications:
      
    		\begin{itemize}
    			\scriptsize	
    			\item Medicine --- Misdiagnosing as healthy vs. having a disease
    			\item (Extreme) Weather prediction ---  Incorrectly predicting that no hurricane occurs 
    			\item Credit granting --- Lending to a risky client vs. not lending to a trustworthy client.
    		\end{itemize}
         
		
		\end{itemize}
        \vspace{15pt}

        \begin{minipage}{0.49\textwidth}
            \begin{table}[]
                \centering
                \begin{tabular}{p{1cm}c|cc}
                    & &\multicolumn{2}{c}{Truth} \\
                    & & Default & Pays Back  \\
                    \hline
                    \multirow{2}{*}{\parbox{1cm}{Pred.}} & Default & 0 & $ 10 $\\
                    & Pays Back & $1000$ & $0$   \\
                \end{tabular}
            \end{table}
        \end{minipage}
        \hfill
        \begin{minipage}{0.49\textwidth}
            \begin{itemize}
                \scriptsize
                \item In these examples, \textbf{the costs of a false negative is much higher than the costs of a false positive}.
                \vspace{15pt}
                
                \item In some applications, the costs are \textbf{unknown} $\leadsto$ need to be specified by experts, or be learnt.
            \end{itemize}   
        \end{minipage}
		
	}
\end{vbframe}


\begin{vbframe}{Cost matrix}
%	
%	
	\begin{itemize}
%		
		\item Input: cost matrix $\mathbf{C}$ 
	\end{itemize}
	%	
	\begin{center}
		\tiny
		\begin{tabular}{cc|>{\centering\arraybackslash}p{8em}>{\centering\arraybackslash}p{8em}>{\centering\arraybackslash}p{5em}>{\centering\arraybackslash}p{8em}}
			& & \multicolumn{4}{c}{\bfseries True Class $y$} \\
			&  & $1$ & $2$ & $\ldots$ & $g$  \\
			\hline
			\bfseries Classification     & $1$ & $C(1,1)$  &  $C(1,2)$  & $\ldots$ &  $C(1,g)$ \\
			& $2$ &  $C(2,1)$  &  $C(2,2)$  & $\ldots$ & $C(2,g)$  \\
            $\yh$ & & & & & \\
			& $\vdots$ & $\vdots$ & $\vdots$ & $\ldots$ & $\vdots$ \\
			& $g$ & $C(g,1)$ & $C(g,2)$  & $\ldots$ &  $C(g,g)$\\
		\end{tabular}
	\end{center}
	%	
	\begin{itemize}
		%		
		\item $C(j,k)$ is the cost of classifying class $k$ as $j,$ 
  \item 0-1-loss would simply be: $C(j,k) = \mathds{1}_{[ j \neq k ]}$
		
		\item $\mathbf{C}$ designed by experts with domain knowledge
		\begin{enumerate}
            \item Too low costs: not enough change in model, still costly errors
            \item Too high costs: might never predict costly classes
		\end{enumerate}
		
	\end{itemize}


\end{vbframe}


\begin{vbframe}{Cost matrix for Imbalanced Learning}
		\begin{itemize}			
	 
			\item Common heuristic for imbalanced data sets: 
	
			\begin{itemize}
                \item $C(j,k) = \frac{n_j}{n_k}$ with $n_k  \ll n_j$,\\
                misclassifying a minority class $k$ as a majority class $j$
		
				\item $C(j,k) = 1$ with $n_j \ll n_k$,\\
    misclassifying a majority class $k$ as a minority class $j$
			
				\item 0 for a correct classification 
			
			\end{itemize}
\vspace{1cm}
    		\item Imbalanced binary classification: \\
            \begin{table}[]
                \centering
                \begin{tabular}{cc|cc}
                    & &\multicolumn{2}{c}{True class} \\
                    & & $y=1$ & $y=-1$  \\
                    \hline
                    \multirow{2}{*}{\parbox{0.5cm}{Pred. class}}& $\hat y$ = 1     & $0$                & $ 1 $\\
                    & $\hat y$ = -1 & $ \frac{n_-}{n_+} $              &  $0$   \\
                \end{tabular}
            \end{table}
    		

            \item So: much higher costs for FNs
        \end{itemize}
		
\end{vbframe}


\begin{vbframe}{Minimum expected Cost Principle}


		\begin{itemize}
		
			\item Suppose we have:
            \begin{itemize}
                \item a cost matrix $\mathbf{C}$
                
                \item knowledge of the true posterior $p(\cdot ~|~ \xv)$
            \end{itemize}

            % \item How to classify given $\xv$: 
            % \begin{itemize}
            %     \footnotesize
            %     \item Compute cost for classifying $\xv$ as $i$. (infeasible to directly compute.)
            %     \vspace{10pt}
                
            %     \item $\leadsto$ Marginalize over ``true'' class $j$.
            %     \vspace{10pt}
            % \end{itemize}

			\item Predict class j with smallest expected costs when marginalizing over true classes:
   %, where the expected costs of a class $i\in\{1,\ldots,g\}$ is
	
			$$ 	\E_{K \sim p(\cdot ~|~ \xv)}( C(j,K) ) = \sumkg p(k ~|~ \xv) C(j,k)	$$
	
		\end{itemize}

    \begin{itemize}
        \item If we trust we trust a probabilistic classifier, we can convert its scores to labels:
        % $f$ which uses a probabilistic score function $\pi:\Xspace \to [0,1]^g$ with $\pi(\xv) = (\pi(\xv)_1,\ldots,\pi(\xv)_g)^\top$ and $\sum_{j=1}^g \pi(\xv)_j = 1$ for the classification, then one can easily modify $h$ to take the expected costs into account:
		$$  \hx := \argminlim_{j=1,\ldots,g} \sumkg 	\pikx C(j,k). $$
 
%        \item For $\xv$, making prediction $i$ means \textbf{acting as if $i$ is the true class of $\xv$.}
        
        \item Can be better to take a less probable class (\href{https://dl.acm.org/doi/10.5555/1642194.1642224}{\beamergotobutton{Elkan et. al. 2001}})

        % \item Now consider: 
        % \begin{itemize}
        %     \footnotesize
        %     \item If we learnt a binary classifier: $p(1 ~|~ \xv) = \pi(\xv)_1$ and $p(-1 ~|~ \xv) = \pi(\xv)_2$,
        %     \item Under what condition should we predict $\xv$ as class $1$?
        % \end{itemize}
    \end{itemize}
\end{vbframe}


\begin{vbframe}{Optimal Threshold for Binary Case}
		\begin{itemize}
            \item Optimal decisions do not change if 
            \begin{itemize}
                \item $\mathbf{C}$ is multiplied by positive constant
                \item $\mathbf{C}$ is added with constant shift
            \end{itemize}

            \item Scale and shift $\mathbf{C}$ to get simpler $\mathbf{C}^\prime$: 
            \begin{table}[]
                \centering
                    \begin{tabular}{cc|cc}
        			& &\multicolumn{2}{c}{True class} \\
        			& & $y=1$ & $y=-1$  \\
        			\hline
        			\multirow{2}{*}{\parbox{0.5cm}{Pred.  class}} & $\hat y$ = 1 & $C^\prime(1,1)$ & $1$ \\
        			& $\hat y$ = -1 & $C^\prime(-1, 1)$ & 0\\
                \end{tabular}
            \end{table}
            where 
            \begin{itemize}
                \item $C^\prime (-1, 1) = \frac{C(-1, 1) - C(-1, -1)}{C(1, -1) - C(-1, -1)}$
                \item $C^\prime (1, 1) = \frac{C(1, 1) - C(-1, -1)}{C(1, -1) - C(-1,-1)}$
            \end{itemize}

            \item We predict $\xv$ as class $1$ if 
            \begin{align*}
                \E_{K \sim p(\cdot ~|~ \xv)}( C^\prime(1,K) )  \leq \E_{K \sim p(\cdot ~|~ \xv)}( C^\prime(-1,K) ) \\
            \end{align*}

			\item Let's unroll the expected value and use $\mathbf{C}^\prime$:
            % need this footnotsize operation, otherwise the first line will be very long.
            \footnotesize{
    			\begin{align*}	
                    & p(-1 ~|~ \xv ) C^\prime(1,-1)  + 	p(1 ~|~ \xv ) C^\prime(1,1) \leq  p(-1 ~|~ \xv ) C^\prime(-1,-1)  + 	p(1 ~|~ \xv ) C^\prime(-1,1)  \\ 
                    &\Rightarrow [1 - p(1 ~|~ \xv)] \cdot 1 + p(1 ~|~ \xv) C^\prime(1, 1) \leq p(1 ~|~ \xv) C^\prime(-1, 1) \\
                    &\Rightarrow p(1 ~|~ \xv) \geq \frac{1}{C^\prime(-1, 1) - C^\prime(1, 1) + 1} \\
                    &\Rightarrow p(1 ~|~ \xv) \geq \frac{C(1, -1) - C(-1, -1)}{C(-1, 1) - C(1, 1) + C(1, -1) - C(-1, -1)} = c^*
    			\end{align*}
            }
		
            \item If even $C(1, 1) = C(-1, -1) = 0 $, we get:
            \begin{align*}
                p(1 ~|~ \xv) \geq \frac{C(1, -1)}{C(-1, 1) + C(1, -1)} = c^{*}
            \end{align*}	
            
            \item Optimal threshold $c^*$ for probabilistic classifier 
            
            $$   \hx := 2 \cdot \mathds{1}_{[ \pi(\xv) \geq c^*]} -1 $$
							
		\end{itemize}
\end{vbframe}



%\begin{vbframe}{MetaCost: Example}
%	%	
%	\small{
%		\begin{itemize}
%			%		
%			\item We compare C4.5 (decision tree) with MetaCost using C4.5 for the \href{http://staffwww.itn.liu.se/~aidvi/courses/06/dm/
%				labs/heart-c.arff}{heart data set}  in \href{ http://www.cs.waikato.ac.nz/ml/weka/}{Weka}. 
%			%		
%			\item The cost matrix $\mathbf{C}$ is 
%			
%			\begin{center}
%				\begin{tabular}{cc|cc}
%					& &\multicolumn{2}{c}{True class} \\
%					& & $y=1$ & $y=-1$  \\
%					\hline
%					\multirow{2}{*}{\parbox{0.3cm}{Pred.  class}}& $\hat y$ = 1     & $0$                & $ 1 $\\
%					& $\hat y$ = -1 & $ 4 $              &  $0$   \\
%				\end{tabular}
%			\end{center}
%		
%			\item The resulting confusion matrices are 
%			
%						
%			\begin{center}
%				\begin{tabular}{cc|cc}
%					& &\multicolumn{2}{c}{True class} \\
%					& MetaCost & $y=1$ & $y=-1$  \\
%					\hline
%					\multirow{2}{*}{\parbox{0.3cm}{Pred.  class}}& $\hat y$ = 1     & $104$                & $ 21 $\\
%					& $\hat y$ = -1 & $ 61 $              &  $117$   \\
%				\end{tabular}
%							\begin{tabular}{cc|cc}
%				& &\multicolumn{2}{c}{True class} \\
%				& C4.5 & $y=1$ & $y=-1$  \\
%				\hline
%				\multirow{2}{*}{\parbox{0.3cm}{Pred.  class}}& $\hat y$ = 1     & $138$                & $ 40 $\\
%				& $\hat y$ = -1 & $ 27$              &  $98$   \\
%			\end{tabular}
%			\end{center}
%		
%		
%			%		
%			\item The total cost of MetaCost is 145, while C4.5 has total costs of 187. However, MetaCost has $0.729$ correct classifications and C4.5 has $0.779.$ 
%			
%			%		
%		\end{itemize}
%		%
%	}
%	%	
%\end{vbframe}



%\begin{vbframe}{Cost-Sensitive Decision Trees}
%	%	
%	\small{
%		\begin{itemize}
%			%		
%			\item 
%			%		
%		\end{itemize}
%		%
%	}
%	%	
%\end{vbframe}


%
\endlecture
\end{document}
