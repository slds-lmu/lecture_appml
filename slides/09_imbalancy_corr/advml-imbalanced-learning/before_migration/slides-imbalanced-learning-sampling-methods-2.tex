\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\usepackage{multicol}

\newcommand{\titlefigure}{figure/SMOTE.png}
\newcommand{\learninggoals}{
	%\item Know the idea of sampling methods for coping with imbalanced data
	\item Understand the state-of-art oversampling technique SMOTE
}

\title{Advanced Machine Learning}
\date{}

\begin{document}
	
\lecturechapter{Imbalanced Learning: \\ Sampling Methods Part 2}
\lecture{Advanced Machine Learning}

\sloppy
	
% \begin{frame}{Sampling Methods: Overview}
%     \begin{itemize}

%         \item Balance training data distribution to perform better on minority classes.
        
%         \item Independent of classifier $\leadsto$ very flexible and general.

%         \item Three groups: 
    
%         \begin{minipage}{0.5\textwidth}
    
%             \begin{itemize} 
                
%                 \item Undersampling --- Removing majority instances.
        
%                 \item Oversampling --- Adding/Creating new minority instances.

%                 \item Oversampling is slower than undersampling but usually works better.

%                 \item Hybrid approaches --- Combining undersampling and oversampling.
        
%             \end{itemize}
    
%         \end{minipage}
%         \begin{minipage}{0.4\textwidth}
%                 \begin{figure}
%                 \centering
%                 \scalebox{0.8}{\includegraphics{figure/under_oversampling.png}}
%             \end{figure}
%         \end{minipage}

%     \end{itemize}
    
% \end{frame}


\begin{frame}{Oversampling: SMOTE}
    \begin{itemize}
        %			
        \item SMOTE creates \textbf{synthetic instances} of minority class.

        \item Interpolate between neighboring minority instances.

        \item Instances are created in $\Xspace$ rather than in $\Xspace\times\Yspace.$

        \item Algorithm: For each minority class instance: 

        \begin{itemize} 

            \item Find its $k$ nearest minority neighbors.
    
            \item Randomly select one of these neighbors.
    
            \item Randomly generate new instances along the lines connecting the minority example and its selected neighbor.
    
        \end{itemize}
    \end{itemize}
    
    \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth]{figure/SMOTE.png} 
    \end{figure}


\end{frame}

\begin{frame}{SMOTE: Generating new examples}
    
    \begin{itemize}

        \item Let $\xi$ be the feature of the minority instance and let $\xv^{(j)}$ be its nearest neighbor. The line connecting the two instances is
        $$		(1-\lambda) \xi + \lambda\xv^{(j)} = \xi + \lambda(\xv^{(j)} - \xi)	$$
        where $\lambda \in [0,1].$		
        
        \item By sampling a $\lambda \in [0,1],$ say $\tilde{\lambda},$ we create a new instance		
        $$   \tilde{\xv}^{(i)} =  \xi + \tilde{\lambda}(\xv^{(j)} - \xi)	 $$

    \end{itemize}		
        
        Example: Let $\xi = (1,2)^\top$ and $\xv^{(j)} = (3,1)^\top.$ Assume $\tilde{\lambda} \approx 0.25.$
        %
    \begin{figure}
        \centering
        \includegraphics[width=0.8\linewidth]{figure_man/coordinate_system}
    \end{figure}

\end{frame}


\begin{frame}{SMOTE: Visualization}
    
    For an imbalanced data situation, take four instances of the minority class. Let $K=2$ be the number of nearest neighbors.		

        \begin{figure}
            \centering
            \foreach \x in{1,2,3,4,5,6,7,8,9,10} {
            \includegraphics<\x>[width=0.8\linewidth]{figure_man/smote_viz_\x.pdf}\par
            }
        \end{figure}
\end{frame}

\begin{frame}{SMOTE: Visualization continued}

    After 100 iterations of SMOTE for $K=2$ we get:		

    \begin{figure}
        \centering
        \includegraphics[width=0.8\linewidth]{figure_man/smote_viz_11.pdf}
    \end{figure}

\end{frame}

\begin{frame}{SMOTE: Visualization continued}	

    After 100 iterations of SMOTE for $K=3$ we get:		

    \begin{figure}
        \centering
        \includegraphics[width=0.8\linewidth]{figure_man/smote_viz_12.pdf}
    \end{figure}
	
\end{frame}
    
\begin{frame}{SMOTE: Example}
    
    \begin{itemize}
        \item Iris data set with 3 classes % $\Yspace=\{ \texttt{setosa},\texttt{versicolor},\texttt{virginica}  \}$, 
        and 50 instances per class.
        
        \item Make the data set ``imbalanced'': 
        \begin{itemize}
            \item relabel one class as positive
            \item relabel two other classes as negative
        \end{itemize}
    \end{itemize}		

    \begin{figure}
        \centering
        \includegraphics[width=0.9\linewidth]{figure_man/smoted_iris_data_ggplot.pdf}
    \end{figure}

SMOTE enriches minority class feature space.
	
\end{frame}


\begin{frame}{SMOTE: Dis-/Advantages}

    \begin{itemize}
    
        \item Generalize decision region for minority class instead of making it quite specific, such as by random oversampling. 
        
        \item Well-performed among the oversampling techniques and is the basis for many oversampling methods: Borderline-SMOTE, LN-SMOTE, $\ldots$ (over 90 extensions!)
    
        \item Prone to overgeneralizing as it pays no attention to majority class.

    \end{itemize}		

\end{frame}

\begin{frame}{Comparison of Sampling Techniques}

    \begin{itemize}
    
        \item Compare different sampling techniques on a binarized version of Optdigits dataset for optical recognition of handwritten digits.
    
        \item Use random forest with 100 trees, 5-fold cv, and $F_1$-Score. %The pos./neg. class-ratios are 0.11, 0.68, 0.68, and 0.79:
           \begin{center}
             \begin{tabular}{lrr}
             \toprule
             Sampling technique & Class ratio &F1-Score\\
             \midrule
             None & 0.11 &0.9239\\
             Undersampling & 0.68 & 0.9538\\
             Oversampling & 0.69& 0.9538\\
             SMOTE & 0.79 & 0.9576\\
             \bottomrule
             \end{tabular}    
            \end{center}
            
        \item Class ratios could be tuned (here done manually).
        \item Sampling techniques outperform base learner. 
        \item SMOTE leads sampling techniques, although by a small margin.

    \end{itemize}		

\end{frame}


\endlecture
\end{document}
