\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-eval}
% Defines macros and environments

\setbeamersize{text margin left=0.3cm,text margin right=0.3cm}


\usepackage{multicol}

\newcommand{\Dtestm}{\mathcal{D}_{\text{test}, m}}
% lowr case c vector used only here, was undefined, not in latex-math
\newcommand{\cv}{\mathbf{c}}

\setbeamersize{text margin left=0.3cm,text margin right=0.3cm}

\title{Applied Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}

\titlemeta{
Imbalanced Data:
}{
Sampling Methods
}{
figure/SMOTE.png
}{
  \item Understand under- and oversampling strategies
  \item Apply SMOTE for synthetic minority class generation
  \item Compare different sampling approaches and their trade-offs
}


%
% https://github.com/slds-lmu/lecture_advml/blob/main/slides/imbalanced-learning/slides-imbalanced-learning-sampling-methods-1.tex
\section{Under- and Oversampling}

\begin{frame}{Sampling Methods: Overview}
    \small{
		\begin{itemize}
			\item Balance training data distribution to perform better on minority classes.
			
			\item Independent of classifier $\leadsto$ very flexible and general.
   
			\item Three groups: 
		
			\begin{minipage}{0.59\textwidth}
				\begin{itemize} 
                    % Need this \small, otherwise the fontsize will be the default size.
                    \small
                    
					\item Undersampling --- Removing instances of majority class(es).
			
					\item Oversampling --- Adding/Creating new instances of minority class(es).  (Slower, but usually works better.)

                    %\item Oversampling is slower, but usually works better.

					\item Hybrid --- Combining both methods.
			
				\end{itemize}
			\end{minipage}
            \hfill
			\begin{minipage}{0.3\textwidth}
					\begin{figure}[c]
					\centering
                    \includegraphics[width=\textwidth]{figure/under_oversampling.png}
				\end{figure}
			\end{minipage}
		\end{itemize}
	}
\end{frame}
	
\begin{frame}{Random Undersampling/Oversampling}

    \begin{itemize}
        \item Random oversampling (ROS):
        \begin{itemize}
            \item Randomly \textbf{replicate} \textbf{minority} instances.% until a desired imbalance ratio.
            \item Prone to overfitting due to multiple tied instances.
        \end{itemize}

        \item Random undersampling (RUS):
        \begin{itemize}
            \item Randomly \textbf{eliminate} \textbf{majority} instances. % until a desired imbalance ratio.
            \item Might remove informative instances and destroy important concepts in data.
        \end{itemize}

        \item Better: Introduce heuristics in removal process (RUS) and do not create exact copies (ROS).
    
        \end{itemize}	

\end{frame}
	
\begin{frame}{Undersampling: Tomek Links}
    \small{

        \begin{itemize}            
            \item Remove ``noisy borderline'' examples (very close observations of different classes) of majority class(es).
%            \item Noisy borderline examples are ``very close'' observations of different classes.
            % \begin{itemize}
            %     % Need this \footnotesize to maintain fontsize
            %     \footnotesize
            %     \item From different classes.
            %     \item ``Very close'' to each other.
            % \end{itemize} 
            \item Let $E^{(i)} = (\xi,\yi)$ and $E^{(j)} = (\xv^{(j)},y^{(j)})$ be two data points in $\D$. % with $\yi\neq y^{(j)}.$
        \end{itemize}
        \begin{columns}
            \begin{column}{0.7\textwidth}	
        
                \begin{itemize}

                    %\vspace{10pt}
                    
                    \item A pair $(E^{(i)},E^{(j)})$ is called \emph{Tomek link} iff there is no other data point $E^{(k)} = (\xv^{(k)},y^{(k)})$ such that
        
                    \begin{itemize} 
                    \footnotesize
                
                       \item [] $d(\xv^{(i)}, \xv^{(k)}) < d(\xv^{(i)}, \xv^{(j)}) $ or
                       \item [] $d(\xv^{(j)}, \xv^{(k)}) < d(\xv^{(i)}, \xv^{(j)}) $ holds,
                    
                    \end{itemize}
        
                where $d$ is some distance on $\Xspace.$
                \item $\yi \neq \yi[j]$% E^{(i)}$ and $E^{(j)}$ have different $y$'s 
                $\leadsto$ noisy borderline examples.

                \item Remove majority instance in each data pair in a Tomek link where $\yi \neq \yi[j]$.

                %\item No random sampling here, but it can be combined with RUS.
                
                \end{itemize}		
            \end{column}
        
            \begin{column}{0.3\textwidth}
                \begin{figure}
                    \centering
                    \includegraphics[width=0.85\textwidth]{figure/tomek_link_plot.png}	
                    \tiny
                    \\ Franciso Herrera (2013), Imbalanced Classification: Common
                    Approaches and Open Problems \furtherreading{HERRERA2013}.
                \end{figure}
            \end{column}
        \end{columns}
   }
\end{frame}

%  \begin{frame}{Undersampling: Tomek Links}

%     \begin{columns}
%         \begin{column}{0.59\textwidth}
%             \begin{itemize} 
                
%                  \item A pair $(E^{(i)},E^{(j)})$ is called \emph{Tomek link} iff there is no other data point $E^{(k)} = (\xv^{(k)},y^{(k)})$ such that $d(\xv^{(i)}, \xv^{(k)}) < d(\xv^{(i)}, \xv^{(j)}) $ or $d(\xv^{(j)}, \xv^{(k)}) < d(\xv^{(i)}, \xv^{(j)}) $ holds.
                
%                 \item $E^{(i)}$ and $E^{(j)}$ have different $y$'s $\leadsto$ a bordeline case.

%                 \item Remove majority instance in each data pair in a Tomek link.

%                 \item No random sampling here, but can be combined with RUS.

            
%             \end{itemize}
%         \end{column}

%         \begin{column}{0.4\textwidth}
%             \begin{figure}
%                 \centering
%                 \includegraphics[width=0.85\textwidth]{figure/tomek_link_plot.png}	\tiny
%                 \\ Franciso Herrera (2013), Imbalanced Classification: Common
%                 Approaches and Open Problems (\href{https://sci2s.ugr.es/sites/default/files/files/TutorialsAndPlenaryTalks/SSTiC-Trends in-Classification-Imbalanced-data-sets.pdf}{\underline{URL}}).
%             \end{figure}
%         \end{column}
%     \end{columns}

% \end{frame}
	
	
% \begin{frame}{Undersampling: Condensed Nearest Neighbor (CNN)}
	
%     \begin{itemize}
    
%         \item Remove majority instances far away from decision boundary. 
%         \item Construct a \textbf{consistent} subset $\tilde{\D}$ of $\D$. % in terms of the 1-NN classifier. 

%         \item A subset $\tilde{\D}$ of $\D$ is called consistent if using a 1-NN classifier on $\tilde{\D}$ classifies each instance in $\D$ correctly.

% %     \end{itemize}	

% % \end{frame}

% % \begin{frame}{Undersampling: Condensed Nearest Neighbor (CNN)}
% %     \begin{itemize}
%         \item Create a consistent subset:
        
%         \begin{enumerate}
    
%             \item Initialize $\tilde{\D}$ by selecting \textbf{all minority} instances and randomly picking \textbf{one majority} instance.
    
%             \item Classify each instance in $\D$ with 1-NN classifier based on $\tilde{\D}.$
        
%             \item Remove all misclassified instances from $\D$.
            
%         \end{enumerate}	
    
%     \end{itemize}
    
    
% \end{frame}
	
\begin{frame}{Undersampling: Other Approaches}

    \begin{itemize}

        \item Neighborhood cleaning rule (NCL):
    
        \begin{enumerate}
            
            \item Find 3 nearest neighbors for each $(\xi,\yi)$ in $\D.$
        
            \item If $\yi$ is majority class \emph{and} 3-NN classifies it as minority $\leadsto$ Remove $(\xi,\yi)$ from $\D$.
        
            \item If $\yi$ is minority class \emph{and} 3-NN classifies it as majority $\leadsto$ Remove 3 nearest neighbors from $\D$.
        
        \end{enumerate} 

    \item Condensed Nearest Neighbor (CNN): Construct a \textbf{minimally consistent} subset $\tilde{\D}$ of $\D$.
        \item One-sided selection (OSS): Tomek link + CNN

        \item CNN + Tomek link: to reduce computation of finding Tomek links $\leadsto$ first use CNN and then remove the Tomek links.
    
        \item Clustering approaches: Class Purity Maximization (CPM) and Undersampling based on Clustering (SBC).

    \end{itemize}

\end{frame}
	
% \begin{frame}{Oversampling: SMOTE}

%     \begin{itemize}	
%         \item The Synthetic Minority Oversampling Technique (SMOTE) operates by creating \textbf{new synthetic examples} of minority class.

%         \item Interpolate between neighboring minority examples.

%         \item Examples are created in $\Xspace$ rather than in $\Xspace\times\Yspace.$

%         \item Algorithm: For each minority instance: 

%         \begin{itemize} 

%             \item Find $k$ nearest minority neighbors.
    
%             \item Randomly select $j$ of these neighbors.
    
%             \item Randomly generate new instances along the lines connecting the minority instance and its $j$ neighbors.
    
%         \end{itemize}

%     \end{itemize}

%     \begin{figure}
%         \centering
%         \includegraphics[width=0.8\textwidth]{figure/SMOTE.png} 
%     \end{figure}

% \end{frame}

% https://github.com/slds-lmu/lecture_advml/blob/main/slides/imbalanced-learning/slides-imbalanced-learning-sampling-methods-2.tex
\section{SMOTE}

% \begin{frame}{Sampling Methods: Overview}
%     \begin{itemize}

%         \item Balance training data distribution to perform better on minority classes.
        
%         \item Independent of classifier $\leadsto$ very flexible and general.

%         \item Three groups: 
    
%         \begin{minipage}{0.5\textwidth}
    
%             \begin{itemize} 
                
%                 \item Undersampling --- Removing majority instances.
        
%                 \item Oversampling --- Adding/Creating new minority instances.

%                 \item Oversampling is slower than undersampling but usually works better.

%                 \item Hybrid approaches --- Combining undersampling and oversampling.
        
%             \end{itemize}
    
%         \end{minipage}
%         \begin{minipage}{0.4\textwidth}
%                 \begin{figure}
%                 \centering
%                 \scalebox{0.8}{\includegraphics{figure/under_oversampling.png}}
%             \end{figure}
%         \end{minipage}

%     \end{itemize}
    
% \end{frame}


\begin{frame}{Oversampling: SMOTE}
    \begin{itemize}
        %			
        \item SMOTE creates \textbf{synthetic instances} of minority class.

        \item Interpolate between neighboring minority instances.

        \item Instances are created in $\Xspace$ rather than in $\Xspace\times\Yspace.$

        \item Algorithm: For each minority class instance: 

        \begin{itemize} 

            \item Find its $k$ nearest minority neighbors.
    
            \item Randomly select one of these neighbors.
    
            \item Randomly generate new instances along the lines connecting the minority example and its selected neighbor.
    
        \end{itemize}
    \end{itemize}
    
    \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth]{figure/SMOTE.png} 
    \end{figure}


\end{frame}

\begin{frame}{SMOTE: Generating new examples}
    
    \begin{itemize}

        \item Let $\xi$ be the feature of the minority instance and let $\xv^{(j)}$ be its nearest neighbor. The line connecting the two instances is
        $$		(1-\lambda) \xi + \lambda\xv^{(j)} = \xi + \lambda(\xv^{(j)} - \xi)	$$
        where $\lambda \in [0,1].$		
        
        \item By sampling a $\lambda \in [0,1],$ say $\tilde{\lambda},$ we create a new instance		
        $$   \tilde{\xv}^{(i)} =  \xi + \tilde{\lambda}(\xv^{(j)} - \xi)	 $$

    \end{itemize}		
        
        Example: Let $\xi = (1,2)^\top$ and $\xv^{(j)} = (3,1)^\top.$ Assume $\tilde{\lambda} \approx 0.25.$
        %
    \begin{figure}
        \centering
        \includegraphics[width=0.8\linewidth]{figure_man/coordinate_system}
    \end{figure}

\end{frame}


\begin{frame}{SMOTE: Visualization}
    
    For an imbalanced data situation, take four instances of the minority class. Let $K=2$ be the number of nearest neighbors.		

        \begin{figure}
            \centering
            \foreach \x in{1,2,3,4,5,6,7,8,9,10} {
            \includegraphics<\x>[width=0.8\linewidth]{figure_man/smote_viz_\x.pdf}\par
            }
        \end{figure}
\end{frame}

\begin{frame}{SMOTE: Visualization continued}

    After 100 iterations of SMOTE for $K=2$ we get:		

    \begin{figure}
        \centering
        \includegraphics[width=0.8\linewidth]{figure_man/smote_viz_11.pdf}
    \end{figure}

\end{frame}

\begin{frame}{SMOTE: Visualization continued}	

    After 100 iterations of SMOTE for $K=3$ we get:		

    \begin{figure}
        \centering
        \includegraphics[width=0.8\linewidth]{figure_man/smote_viz_12.pdf}
    \end{figure}
	
\end{frame}
    
\begin{frame}{SMOTE: Example}
    
    \begin{itemize}
        \item Iris data set with 3 classes % $\Yspace=\{ \texttt{setosa},\texttt{versicolor},\texttt{virginica}  \}$, 
        and 50 instances per class.
        
        \item Make the data set ``imbalanced'': 
        \begin{itemize}
            \item relabel one class as positive
            \item relabel two other classes as negative
        \end{itemize}
    \end{itemize}		

    \begin{figure}
        \centering
        \includegraphics[width=0.9\linewidth]{figure_man/smoted_iris_data_ggplot.pdf}
    \end{figure}

SMOTE enriches minority class feature space.
	
\end{frame}


\begin{frame}{SMOTE: Dis-/Advantages}

    \begin{itemize}
    
        \item Generalize decision region for minority class instead of making it quite specific, such as by random oversampling. 
        
        \item Well-performed among the oversampling techniques and is the basis for many oversampling methods: Borderline-SMOTE, LN-SMOTE, $\ldots$ (over 90 extensions!)
    
        \item Prone to overgeneralizing as it pays no attention to majority class.

    \end{itemize}		

\end{frame}

\begin{frame}{Comparison of Sampling Techniques}

    \begin{itemize}
    
        \item Compare different sampling techniques on a binarized version of Optdigits dataset for optical recognition of handwritten digits.
    
        \item Use random forest with 100 trees, 5-fold cv, and $F_1$-Score. %The pos./neg. class-ratios are 0.11, 0.68, 0.68, and 0.79:
           \begin{center}
             \begin{tabular}{lrr}
             \toprule
             Sampling technique & Class ratio &F1-Score\\
             \midrule
             None & 0.11 &0.9239\\
             Undersampling & 0.68 & 0.9538\\
             Oversampling & 0.69& 0.9538\\
             SMOTE & 0.79 & 0.9576\\
             \bottomrule
             \end{tabular}    
            \end{center}
            
        \item Class ratios could be tuned (here done manually).
        \item Sampling techniques outperform base learner. 
        \item SMOTE leads sampling techniques, although by a small margin.

    \end{itemize}		

\end{frame}

\section{Conclusion}

\begin{frame}{When to counteract imbalanced data?}
\begin{itemize}
    \item Only counteract if your metric is impacted by imbalanced data
    \item How to counteract? Can you change to a metric that is not affected by imbalanced data?
    \item Check if treatment of imbalanced data has any adversarial effects \furtherreading{BISCHL2024-ADVERSARIAL}
    \item Try simple methods first, especially SMOTE is highly criticized \furtherreading{VANHULSE2022-SMOTE}
    \item Use hyperparameter optimization to decide what method to use.
    \item Why not treat finding the trade-off between precision and recall as a multi-objective hyperparameter optimization problem?
\end{itemize}
    
\end{frame}

\endlecture
\end{document}