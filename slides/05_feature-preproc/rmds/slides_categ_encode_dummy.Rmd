```{r include=FALSE, cache=FALSE}
library(knitr)
library(dplyr)
root = rprojroot::find_root(rprojroot::is_git_root)
ap = adjust_path(paste0(getwd(), "/figure"))
```

## Categorical Features

A categorical feature is a feature with a finite number of discrete (unordered) *levels* $c_1, \dots, c_k$, e.g., *House.Style=2Story*$\stackrel{?}{>}$*SFoyer*.

- Categorical features are very common in practical applications.

- Except for few machine learning algorithms like tree-based methods, categorical features have to be encoded in a preprocessing step.

*Encoding* is the creation of a fully numeric representation from a categorical feature.

- Choosing the optimal encoding can be a challenge, especially when the number of levels $k$ becomes very large.

## One-Hot Encoding

- Convert each categorical feature to $k$ binary ($1/0$) features, where $k$ is the number of unique levels.

- One-Hot encoding does not loose any information of the feature and many models can correctly handle binary features.

- Given a categorical feature $x_j$ with levels $c_1,\dots, c_k$, the new features are
$$
  \tilde x_{j,c} = \I(x_j)_{c} \quad c = c_1,\dots,c_k.
$$

__One-Hot encoding is often the go-to choice for the encoding of categorical features!__


## One-Hot Encoding: Example

Original slice of the dataset:

```{r, echo=FALSE, message=FALSE}
library(mlr)
data = read.csv(paste0(root, "/data/ames_housing_extended.csv"), stringsAsFactors = TRUE)

data %>%
  select(SalePrice, Central.Air, Bldg.Type) %>%
  slice(5:9) %>%
  knitr::kable(format = 'latex', booktabs = TRUE) %>%
  kableExtra::kable_styling(latex_options = 'HOLD_position', font_size = 7)
```

One-Hot Encoded:

```{r, echo=FALSE}
data %>%
  select(SalePrice, Central.Air, Bldg.Type) %>%
  slice(5:9) %>%
  createDummyFeatures(target = "SalePrice") %>%
  knitr::kable(format = 'latex', booktabs = TRUE) %>%
  kableExtra::kable_styling(latex_options = 'HOLD_position', font_size = 4)
```

## Dummy Encoding

- Dummy encoding is very similar to one-hot encoding with the difference that only $k-1$ binary features are created.

- A *reference* category is defined as all binary features being $0$, i.e.,
$$
\tilde x_{j,1} = 0, \dots, \tilde x_{j,k-1} = 0.
$$
- Each feature $\tilde x_{j,1}$ represents the _deviation_ from the reference category.

- While using a reference category is required for stability and interpretability in statistical models like (generalized) linear models, it is not necessary, rarely done in ML and can even have negative influence on the performance.

## Ames Housing - One-Hot vs. Dummy Encoding

```{r, echo=FALSE, message=FALSE, fig.height = 4.5}
library(tidyverse)
library(mlr)
library(mlrCPO)

task = data %>%
  select(SalePrice, MS.Zoning, Street, Lot.Shape, Land.Contour, Bldg.Type) %>%
  makeRegrTask(id = "None", target = "SalePrice") %>>% cpoFixFactors()

task1 = createDummyFeatures(task, method = "1-of-n")
task1$task.desc$id = "One-Hot"


task2 = createDummyFeatures(task, method = "reference")
task2$task.desc$id = "Dummy"

lrns =  list(
  makeLearner(id = "Linear Regression", "regr.lm"),
  makeLearner(id = "Random Forest", "regr.ranger"))

set.seed(1)
rin = makeResampleInstance(cv10, task1)

res = benchmark(lrns, list(task1, task2, task), rin, mae)
as.data.frame(res) %>%
  filter(task.id != "None" | learner.id != "Linear Regression") %>%
  ggplot(aes(y = mae, x = task.id)) + geom_boxplot() + facet_wrap(~learner.id) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 20, hjust = 1)) +
  ylab("Mean Absolute Error") +
  xlab("") + ggtitle("Ames House Price Prediction")
```

- Result of linear model depends on actual implementation, e.g., R's `lm()` produces a *rank-deficient fit* warning and recovers by dropping the intercept.

## One-Hot Encoding: Limitations

- One-Hot encoding can become extremely inefficient when number of levels becomes too large, as one additional feature is introduced for every level.

- Assume a categorical feature with $k=4000$ levels, by using dummy encoding 4000 new features are added to the dataset.

- These additional features are very sparse.

- Handling such *high-cardinality categorical features* is a challenge, possible solutions are

    - specialized methods such as _factorization machines_,
    - __target/impact encoding__,
    - clustering feature levels or
    - feature hashing.
