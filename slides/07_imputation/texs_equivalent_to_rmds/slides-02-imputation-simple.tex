\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\title{Applied Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document} 

\titlemeta{
Imputation:
}{
Simple Methods
}{
figure_man/fe_imputation_simple
}{
\item Simple imputation strategies
\item Disadvantages of constant imputation
\item Imputation by sampling
}

\begin{frame}{Simple Imputation Methods}

    A very simple imputation strategy is to replace missing values with univariate statistics, e.g. mean or median, of the feature:
    
    \begin{center}
        \includegraphics[width=\textwidth]{figure_man/fe_imputation_simple.pdf}
    \end{center}

\end{frame}

\begin{frame}{Simple Imputation Methods}

    The statistic used to impute the missing values has to match the type of the feature:
    
    \begin{itemize}
        \item Numeric features: mean, median, quantiles, mode, ...
        \item Categorical features: mode, ...
    \end{itemize}
    
    Alternatively missing values can be encoded with new values
    
    \begin{itemize}
        \item Numeric features: \texttt{2*max}, ...
        \item Categorical features: \texttt{\_\_MISS\_\_}, ...
    \end{itemize}

\end{frame}

\begin{frame}{Imputation - Notes}

    \begin{itemize}
        \item To ensure that the information regarding which values were imputed is not lost, we can add a binary indicator variable.
        
        \item Domain knowledge is highly important: Missing \textit{Credit} can mean that the individual has \$0 debt.
        
        \item Encoding numeric values with \textit{out-of-range} values has been shown to work well in practice for complex ML models.
        \begin{itemize}
            \item This is especially useful for tree-based methods, as it allows separating observations with missing values in a feature.
            \item But using \textit{out-of-range} imputation when estimating global effects (e.g. in linear models) can skew the results
        \end{itemize}
    \end{itemize}

\end{frame}

\begin{frame}{Disadvantage of Constant Imputation}

    By imputing a feature with one value we shift the distribution of that feature towards a single value.
    
    \begin{center}
        \includegraphics[width=0.7\textwidth]{figure/imputation_distribution_comparison}
    \end{center}

\end{frame}

\begin{frame}{Imputation by Sampling}

    A way out of this problem is to sample values to replace each missing observation from
    
    \begin{itemize}
        \item the empirical distribution or histogram, for a numeric feature.
        \item the relative frequencies of levels, for a categorical feature.
    \end{itemize}
    
    This ensures that the distribution of the features does not change much.

\end{frame}

\begin{frame}{Benchmark of Simple Imputation}

    To illustrate the effect of imputation on the performance we evaluate a linear model on the Ames housing dataset.
    Evaluation is done with a 10-fold cross-validation:
    
    \begin{center}
        \includegraphics[width=0.7\textwidth]{figure/imputation_benchmark_comparison}
    \end{center}

\end{frame}

\endlecture
\end{document}
