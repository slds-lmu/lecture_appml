\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Applied Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\setbeamersize{text margin left=0.3cm,text margin right=0.3cm}


\begin{document}

\titlemeta{
Feature Selection:
}{
Filter Methods
}{
figure_man/empty
}{
    \item Correlation-based Filtering
    \item AUC/ROC-based Filtering
}


\section{Filtering}


% \includepdf[pages={2},trim=0 20 0 0,clip,width=490pt,offset=-10pt 10pt,pagecommand={}]{sl/slides-fs-filters1.pdf} 
% pages 2, 4, 5, 7

\begin{frame2}{Introduction}
  \vspace{0.4cm}
  \begin{itemize}
  \setlength{\itemsep}{0.8em}
    \item \textbf{Filter methods} construct a measure that quantifies the dependency between features and the target variable
    \item They yield a numerical score for each feature $x_j$, according to which we rank the features
    \item They are model-agnostic and can be applied generically
    %\item Filter methods are strongly related to methods for determining variable importance.
  \end{itemize}
  \vspace{-0.2cm}
  \begin{center}
  \includegraphics[width=0.55\textwidth]{sl/feature-selection/figure/fs-auc-barplot.png}\\
  \footnotesize{Exemplary filter score ranking for Spam data}
  \end{center}
  
\end{frame2}

% 4, 5

\begin{frame2}{Pearson \& Spearman correlation}
\textbf{Pearson correlation $r(x_j, y)$: }
\begin{itemize}
\item For numeric features and targets only
\item Measures linear dependency
\item $ r(x_j, y) = \frac{\sum_{i=1}^n (x^{(i)}_j - \bar{x}_j) (\yi - \bar{y})}{\sqrt{\sum_{i=1}^n (x^{(i)}_j - \bar{x}_j)} \sqrt{(\sum_{i=1}^n \yi - \bar{y})}},\qquad -1 \le r \le 1$
\end{itemize}
\vspace{0.4cm}
\textbf{Spearman correlation $r_{SP}(x_j, y)$:}
\begin{itemize}
\item For features and targets at least on ordinal scale
\item Equivalent to Pearson correlation computed on ranks
\item Assesses monotonicity of relationship
% \item Calculate the Spearman correlation coefficient between each feature-target combination:
% $$ r_{SP} = \frac{\sum (rg(\xi) - \bar{rg}_x) (rg(\yi) - \bar{rg}_y)}{\sqrt{\sum (rg(\xi) - \bar{rg}_x)^2 \sum (rg(\yi) - \bar{rg}_y)^2}}$$
% $-1 \le r_{SP} \le 1$
% 
% $r_{SP} > 0$: positive correlation.
% 
% $r_{SP} < 0$: negative correlation.
% 
% $r_{SP} = 0$: no correlation.
% \item A higher score indicates a higher relevance of the feature.
\end{itemize}
\lz
Use absolute values $|r(x_j, y)|$ for feature ranking:\\
higher score indicates a higher relevance

\end{frame2}
\begin{frame2}{Pearson \& Spearman correlation}

Only \textbf{linear} dependency structure, non-linear (non-monotonic) aspects are not captured:

\lz

% in rsrc/chunk2_filter_correlation.R created
\begin{center}
\includegraphics[width=0.75\textwidth]{sl/feature-selection/figure_man/correlation_example.png}\\
\footnotesize{Comparison of Pearson correlation for different dependency structures.}
\end{center}
%Comparison of Pearson correlation for different dependency structures.\\
\vspace{0.1cm}
To assess strength of non-linear/non-monotonic dependencies, generalizations such as \textbf{distance correlation} can be used.

% \begin{center}
%   \includegraphics[width=0.9\textwidth]{figure_man/correlation_example.png}
% 
%   \scriptsize{\url{https://en.wikipedia.org/wiki/Pearson\_correlation\_coefficient\#/media/File:Correlation\_examples2.svg}}
% \end{center}
% 
% \begin{vbframe}{Filter: Rank correlation}
% \begin{itemize}
%   \item For features and targets at least on ordinal scale.
%   \item Equivalent to Pearson correlation computed on the ranks.
%   \item Assesses monotonicity of the dependency relationship.
% \item Calculate the Spearman correlation coefficient between each feature-target combination:
% $$ r_{SP} = \frac{\sum (rg(\xi) - \bar{rg}_x) (rg(\yi) - \bar{rg}_y)}{\sqrt{\sum (rg(\xi) - \bar{rg}_x)^2 \sum (rg(\yi) - \bar{rg}_y)^2}}$$
% $-1 \le r_{SP} \le 1$
% 
% $r_{SP} > 0$: positive correlation.
% 
% $r_{SP} < 0$: negative correlation.
% 
% $r_{SP} = 0$: no correlation.
% \item A higher score indicates higher relevance of the feature.
%   \end{itemize}
% \end{vbframe}
\end{frame2}


% 7
\begin{frame2}{AUC/ROC}
\begin{itemize}
\item For binary classification with $\Yspace = \{ 0, 1\}$ and numeric features
\item Classify samples using single feature (with thresholds), compute AUC per feature as proxy for its ability to separate classes 
%\item Uses each feature's values to classify samples for different thresholds, computing AUC per feature as proxy for its ability to separate classes.
\item Features are then ranked; higher AUC scores $\to$ higher relevance.
\end{itemize}
%Text explaining the AUC feature selection method here (calculate ROC curve thresholded on each individual feature, larger the better,...)
\begin{center}
\begin{figure}
    \includegraphics[width=0.7\textwidth]{sl/feature-selection/figure/fs-roc-curve.png}
\end{figure}

%\footnotesize{Isabelle Guyon, AndrÃ© Elisseeff (2003). An Introduction to Variable and Feature Selection.  Journal of Machine Learning Research (3) p. 1157-1182.}
\end{center}
\end{frame2}




\begin{frame}{Further Criteria/Methods}
    \begin{itemize}
        \item Classification
        \begin{itemize}
            \item $\chi^2$-statistic
            \item Welch's t-Test
            \item F-Test
            
        \end{itemize}
        \item Regression \& Classification
        \begin{itemize}
            \item Mutual Information
            \item Relief
        \end{itemize}
        \item Model-based
        \begin{itemize}
            \item Tree-based feature importance (decision tree, extra trees, random forest)
            \item Coefficients of linear model
        \end{itemize}
    \end{itemize}
\end{frame}

% \includepdf[pages={5},trim=0 20 0 0,clip,width=490pt,offset=0pt 10pt,pagecommand={}]{sl/slides-fs-filters2.pdf} 
\begin{vbframe}{Using Filter Methods}

% \begin{minipage}{0.45\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figure/guyon_example_xor.png} % second figure itself
%     %\caption{second figure}
% \end{minipage}

\begin{columns}
\begin{column}{0.65\textwidth}
    \begin{enumerate}{}
    \setlength{\itemsep}{1.2em}
        \item Calculate filter score for each feature $x_j$
        \item Rank features according to score values
        \item Choose $\tilde{p}$ best features
        \item Train model on $\tilde{p}$ best features
    \end{enumerate}

    \begin{blocki}{How to choose $\tilde{p}$?}
        \item Could be prescribed by application
        \item Eyeball estimation: read from filter plots
        \item Treat as hyperparameter and tune in a pipeline, based on resampling
    \end{blocki}
\end{column}

\begin{column}{0.46\textwidth}
    %\vspace{1cm}
    \begin{figure}
    \centering
    \includegraphics[width=0.89\textwidth]{sl/feature-selection/figure/filter_comparison_har_classif.kknn.png}
    \end{figure}
    \vspace{-0.4cm}
    \begin{figure}
    \includegraphics[width=0.89\textwidth]{sl/feature-selection/figure/fs-filters-scree-plot.png}
    \end{figure}
\end{column}

\end{columns}
\end{vbframe}

\begin{vbframe}{Advantages and Disadvantages}
    Advantages
    \begin{itemize}
        \item Fast
        \item Typically scales well with the number of features $p$
        \item Generally interpretable
        \item Can be used with any inducer
    \end{itemize}
    Negative
    \begin{itemize}
        \item Does not take specifics of the inducing algorithm into account
        \item Redundant features will have similar weights
        \item Often univariate
    \end{itemize}
\end{vbframe}

\endlecture
\end{document}
