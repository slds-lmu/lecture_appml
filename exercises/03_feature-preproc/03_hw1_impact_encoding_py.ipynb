{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning: Homework Exercise 03-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Apply what you have learned about using pipelines for efficient pre-processing and model training on a regression problem.\n",
    "\n",
    "## House Prices in King county\n",
    "\n",
    "In this exercise, we want to model house sale prices in King county in the state of Washington, USA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  bedrooms  bathrooms  sqft_living  sqft_lot  floors  \\\n",
       "0  20141013T000000         3       1.00         1180      5650     1.0   \n",
       "1  20141209T000000         3       2.25         2570      7242     2.0   \n",
       "2  20150225T000000         2       1.00          770     10000     1.0   \n",
       "3  20141209T000000         4       3.00         1960      5000     1.0   \n",
       "4  20150218T000000         3       2.00         1680      8080     1.0   \n",
       "\n",
       "   waterfront  view  condition  grade  sqft_above  sqft_basement  yr_built  \\\n",
       "0           0     0          3      7        1180              0      1955   \n",
       "1           0     0          3      7        2170            400      1951   \n",
       "2           0     0          3      6         770              0      1933   \n",
       "3           0     0          5      7        1050            910      1965   \n",
       "4           0     0          3      8        1680              0      1987   \n",
       "\n",
       "   yr_renovated zipcode      lat     long  sqft_living15  sqft_lot15  \n",
       "0             0   98178  47.5112 -122.257           1340        5650  \n",
       "1          1991   98125  47.7210 -122.319           1690        7639  \n",
       "2             0   98028  47.7379 -122.233           2720        8062  \n",
       "3             0   98136  47.5208 -122.393           1360        5000  \n",
       "4             0   98074  47.6168 -122.045           1800        7503  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "rng = np.random.default_rng(124)\n",
    "\n",
    "X, y = fetch_openml(data_id=42092, return_X_y=True)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do some simple feature pre-processing first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21613 entries, 0 to 21612\n",
      "Data columns (total 17 columns):\n",
      " #   Column         Non-Null Count  Dtype   \n",
      "---  ------         --------------  -----   \n",
      " 0   date           21613 non-null  int64   \n",
      " 1   bedrooms       21613 non-null  int64   \n",
      " 2   bathrooms      21613 non-null  float64 \n",
      " 3   sqft_living    21613 non-null  int64   \n",
      " 4   sqft_lot       21613 non-null  int64   \n",
      " 5   floors         21613 non-null  float64 \n",
      " 6   waterfront     21613 non-null  category\n",
      " 7   view           21613 non-null  int64   \n",
      " 8   condition      21613 non-null  int64   \n",
      " 9   grade          21613 non-null  int64   \n",
      " 10  sqft_above     21613 non-null  int64   \n",
      " 11  yr_built       21613 non-null  int64   \n",
      " 12  zipcode        21613 non-null  category\n",
      " 13  lat            21613 non-null  float64 \n",
      " 14  long           21613 non-null  float64 \n",
      " 15  sqft_living15  21613 non-null  int64   \n",
      " 16  sqft_lot15     21613 non-null  int64   \n",
      "dtypes: category(2), float64(4), int64(11)\n",
      "memory usage: 2.5 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# To be consistent with mlr3data package, we need the following steps [1-4]. \n",
    "# These steps are not shown in the R solution, as they are done in the mlr3data.\n",
    "# For details please see https://github.com/mlr-org/mlr3data/blob/main/R/kc_housing.R\n",
    "\n",
    "# 1. Convert dates from strings to datetime\n",
    "X['date'] = pd.to_datetime(X['date'])\n",
    "\n",
    "# 2. Replace 0 values with NA in yr_renovated\n",
    "X['yr_renovated'] = X['yr_renovated'].replace(0, np.nan)\n",
    "\n",
    "# 3. Replace 0 values with NA in sqft_basement\n",
    "X['sqft_basement'] = X['sqft_basement'].replace(0, np.nan)\n",
    "\n",
    "# 4. Convert waterfront to category.\n",
    "X['waterfront'] = X['waterfront'].astype('category')\n",
    "\n",
    "# Next, we do the feature preprocessing\n",
    "\n",
    "# Convert zipcode to category\n",
    "X['zipcode'] = X['zipcode'].astype('category')\n",
    "\n",
    "# Transform date to numeric variable (days since earliest date)\n",
    "min_date = X['date'].min()\n",
    "X['date'] = (X['date'] - min_date).dt.days\n",
    "\n",
    "# Scale the target variable by dividing by 1000\n",
    "y = y.astype(float) / 1000\n",
    "\n",
    "# Delete columns containing NAs\n",
    "X = X.drop(X.columns[X.isna().any()], axis=1)\n",
    "\n",
    "# Print information about the DataFrame\n",
    "print(\"DataFrame info:\")\n",
    "print(X.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "Before we train a model, let’s reserve some data for evaluating our model later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (12967, 17)\n",
      "Test set shape: (8646, 17)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data with a 60% training ratio.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, random_state=124)\n",
    "\n",
    "# Print out the shapes of the training and testing sets.\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "XGBoost (Chen and Guestrin, 2016) is a highly performant library for gradient-boosted trees. As with some other ML algorithms, XGBoost in scikit-learn does not natively handle categorical data, meaning categorical features must be encoded as numerical variables before use. \n",
    "\n",
    "In the King County dataset, there are two categorical features: **\"waterfront\"** and **\"zipcode\"**. Categorical features can be grouped by their cardinality, which refers to the number of unique levels they contain: binary features (two levels), low-cardinality features, and high-cardinality features. There is no universal threshold defining when a feature is considered high-cardinality; such a threshold can even be tuned based on performance. \n",
    "\n",
    "Low-cardinality features, such as binary features, can typically be handled by **one-hot encoding**. One-hot encoding converts categorical features into a binary representation, where each possible category is represented as a separate binary feature. While theoretically, it's sufficient to create one less binary feature than levels (often called dummy or treatment encoding, particularly relevant for generalized linear models), scikit-learn's default `OneHotEncoder` usually creates a full set of binary features unless specified otherwise.\n",
    "\n",
    "For this dataset:\n",
    "\n",
    "- **\"waterfront\"** has **2** unique levels, making it a binary feature suitable for one-hot (or dummy) encoding.\n",
    "- **\"zipcode\"** has **70** unique levels, making it a very high-cardinality feature.\n",
    "\n",
    "High-cardinality categorical features, like `\"zipcode\"`, can be problematic if encoded with methods like one-hot encoding due to dimensionality explosion, and might require alternative encoding methods or preprocessing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact encoding\n",
    "\n",
    "Impact encoding (Micci-Barreca 2001) is a good approach for handling high-cardinality features. Impact encoding converts categorical features into numeric values. The idea behind impact encoding is to use the target feature to create a mapping between the categorical feature and a numerical value that reflects its importance in predicting the target feature. Impact encoding involves the following steps:\n",
    "\n",
    "1. Group the target variable by the categorical feature.\n",
    "2. Compute the mean of the target variable for each group.\n",
    "3. Compute the global mean of the target variable.\n",
    "4. Compute the impact score for each group as the difference between the mean of the target variable for the group and the global mean of the target variable.\n",
    "5. Replace the categorical feature with the impact scores.\n",
    "\n",
    "Impact encoding preserves the information of the categorical feature while also creating a numerical representation that reflects its importance in predicting the target. Compared to one-hot encoding, the main advantage is that only a single numeric feature is created regardless of the number of levels of the categorical features, hence it is especially useful for high-cardinality features. As information from the target is used to compute the impact scores, the encoding process must be embedded in cross-validation to avoid leakage between training and testing data.\n",
    "\n",
    "`sklearn` has implemented impact encoding in the class `sklearn.preprocessing.TargetEncoder`. Please refer to [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Create a pipeline\n",
    "\n",
    "Create a pipeline that preprocesses each categorical variable using impact encoding. The pipeline should include hyperparameter optimization with an XGBoost model, using randomized search and mean squared error (MSE) as the performance measure. You should perform hyperparameter tuning via cross-validation with a number of folds `cv=2`.\n",
    "\n",
    "For the hyperparameter search space, define ranges informed by empirical best practices and common defaults for tuning XGBoost:\n",
    "\n",
    "- `learning_rate`: sample from a log-uniform distribution between 1e-4 and 1.\n",
    "- `max_depth`: sample integer values from 1 to 20.\n",
    "- `colsample_bytree` and `colsample_bylevel`: uniform distributions from 0.1 to 1.0.\n",
    "- `reg_lambda` and `reg_alpha`: log-uniform distributions between 0.001 and 1000.\n",
    "- `subsample`: uniform distribution from 0.1 to 1.0.\n",
    "\n",
    "To speed up computations, set `n_estimators=100` and enable multi-core computing (`n_jobs=-1`) in your `XGBRegressor`.\n",
    "\n",
    "Additionally, you will compare the pipeline using impact encoding with an alternative pipeline using one-hot encoding, keeping all other pipeline steps and hyperparameter tuning procedures identical. This will allow you to explore how different encoding strategies affect the model's predictive performance.\n",
    "\n",
    "<details><summary>Hint 1:</summary>\n",
    "    Use `loguniform`, `uniform`, `randint` from `scipy.stats` for specifying the distributions in parameter search.\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2:</summary>\n",
    "    Use `objective='reg:squarederror'` in `XGBRegressor`.\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 3:</summary>\n",
    "    Use `neg_mean_squared_error` in `RandomizedSearchCV`.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#===SOLUTION===\n",
    "\n",
    "from scipy.stats import loguniform, uniform, randint\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, TargetEncoder\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "# Define the search space for the XGBoost hyperparameters\n",
    "param_distributions = {\n",
    "    'xgb__learning_rate': loguniform(1e-4, 1.0),         # eta: from 1e-4 to 1 (log scale)\n",
    "    'xgb__max_depth': randint(1, 21),                    # max_depth: from 1 to 20\n",
    "    'xgb__colsample_bytree': uniform(0.1, 0.9),          # colsample_bytree: from 0.1 to 1.0\n",
    "    'xgb__colsample_bylevel': uniform(0.1, 0.9),         # colsample_bylevel: from 0.1 to 1.0\n",
    "    'xgb__reg_lambda': loguniform(0.001, 1000),          # reg_lambda: from 0.001 to 1000 (log scale)\n",
    "    'xgb__reg_alpha': loguniform(0.001, 1000),           # reg_alpha: from 0.001 to 1000 (log scale)\n",
    "    'xgb__subsample': uniform(0.1, 0.9)                  # subsample: from 0.1 to 1.0\n",
    "}\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = ['zipcode', 'waterfront']\n",
    "numerical_features = [col for col in X_train.columns if col not in categorical_features]\n",
    "\n",
    "# 1. Pipeline with TargetEncoder\n",
    "target_preprocessor = ColumnTransformer([\n",
    "    ('target_encoder', TargetEncoder(smooth=0.0001), categorical_features),\n",
    "    ('passthrough', 'passthrough', numerical_features)\n",
    "])\n",
    "\n",
    "target_pipeline = Pipeline([\n",
    "    ('preprocessor', target_preprocessor),\n",
    "    ('xgb', XGBRegressor(n_estimators=100, n_jobs=-1, objective='reg:squarederror', verbosity=0))\n",
    "])\n",
    "\n",
    "# 2. Pipeline with OneHotEncoder\n",
    "onehot_preprocessor = ColumnTransformer([\n",
    "    ('onehot_encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features),\n",
    "    ('passthrough', 'passthrough', numerical_features)\n",
    "])\n",
    "\n",
    "onehot_pipeline = Pipeline([\n",
    "    ('preprocessor', onehot_preprocessor),\n",
    "    ('xgb', XGBRegressor(n_estimators=100, n_jobs=-1, objective='reg:squarederror', verbosity=0))\n",
    "])\n",
    "\n",
    "# Create RandomizedSearchCV for TargetEncoder pipeline\n",
    "target_search = RandomizedSearchCV(\n",
    "    estimator=target_pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=2,\n",
    "    random_state=111,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Create RandomizedSearchCV for OneHotEncoder pipeline\n",
    "onehot_search = RandomizedSearchCV(\n",
    "    estimator=onehot_pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=2,\n",
    "    random_state=124,\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Benchmark a pipeline\n",
    "\n",
    "Assess the performance of both pipelines using the untouched test set principle:\n",
    "\n",
    "- Perform CV on your training dataset (`X_train`, `y_train`).\n",
    "- Evaluate their predictive performance (MSE) on the separate test set (`X_test`, `y_test`).\n",
    "\n",
    "Finally, report and compare the optimal hyperparameters and corresponding performance metrics obtained from cross-validation and on the independent test set.\n",
    "\n",
    "<details><summary>Hint 1:</summary>\n",
    "    Use `RandomizedSearchCV.best_score_` to see the best validation score, and use `RandomizedSearchCV.score` to run performance evaluation on the test set.\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Hint 2:</summary>\n",
    "    Recall that the `RandomizedSearchCV` use negative MSE as the score in the HPO. Therefore, when printing the performances on the validation and test sets, you need to flip the sign of the score.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model with OneHotEncoder...\n",
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n",
      "Training model with TargetEncoder...\n",
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n",
      "\n",
      "--- OneHotEncoder Results ---\n",
      "Best parameters: {'xgb__colsample_bylevel': 0.9739842882294312, 'xgb__colsample_bytree': 0.2816306321316162, 'xgb__learning_rate': 0.24734640788083925, 'xgb__max_depth': 4, 'xgb__reg_alpha': 63.31327250747733, 'xgb__reg_lambda': 0.014633103639863188, 'xgb__subsample': 0.8480834882645039}\n",
      "Best CV score (MSE): 17334.5468\n",
      "Test set score (MSE): 16713.4028\n",
      "\n",
      "--- TargetEncoder Results ---\n",
      "Best parameters: {'xgb__colsample_bylevel': 0.8656327252373124, 'xgb__colsample_bytree': 0.5827666592066569, 'xgb__learning_rate': 0.07834804904588204, 'xgb__max_depth': 8, 'xgb__reg_alpha': 57.53289051844459, 'xgb__reg_lambda': 0.8489426245140799, 'xgb__subsample': 0.658346139517151}\n",
      "Best CV score (MSE): 18180.8702\n",
      "Test set score (MSE): 14889.9817\n"
     ]
    }
   ],
   "source": [
    "#===SOLUTION===\n",
    "\n",
    "print(\"\\nTraining model with OneHotEncoder...\")\n",
    "onehot_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training model with TargetEncoder...\")\n",
    "target_search.fit(X_train, y_train)\n",
    "\n",
    "# Output results for OneHotEncoder\n",
    "print(\"\\n--- OneHotEncoder Results ---\")\n",
    "print(f\"Best parameters: {onehot_search.best_params_}\")\n",
    "print(f\"Best CV score (MSE): {-onehot_search.best_score_:.4f}\")\n",
    "onehot_test_score = onehot_search.score(X_test, y_test)\n",
    "print(f\"Test set score (MSE): {-onehot_test_score:.4f}\")\n",
    "\n",
    "# Output results for TargetEncoder\n",
    "print(\"\\n--- TargetEncoder Results ---\")\n",
    "print(f\"Best parameters: {target_search.best_params_}\")\n",
    "print(f\"Best CV score (MSE): {-target_search.best_score_:.4f}\")\n",
    "target_test_score = target_search.score(X_test, y_test)\n",
    "print(f\"Test set score (MSE): {-target_test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We learned how to apply pre-processing steps together with tuning to construct refined pipelines for benchmark experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
