{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning: In-class Exercise 04-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "After this exercise, you should understand and be able to perform feature selection using wrapper methods with `scikit-learn`. You should also be able to integrate various performance measures and calculate the generalization error.\n",
    "\n",
    "## Wrapper Methods\n",
    "\n",
    "In addition to filtering, wrapper methods are another variant of feature selection. While filtering methods apply predefined criteria directly to feature values, wrapper methods evaluate different subsets of features by repeatedly training and validating a predictive model. Since models need to be repeatedly refitted, this method can be computationally expensive.\n",
    "\n",
    "For wrapper methods in Python, we will primarily use the classes from `sklearn.feature_selection`, e.g., `SequentialFeatureSelector`. This approach incrementally adds or removes features based on model performance.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "We first load the necessary Python libraries and set a fixed random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we use the `credit-g` dataset (German Credit Data) and a random forest classifier (`RandomForestClassifier`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml('credit-g', version=1, as_frame=True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Basic Application\n",
    "\n",
    "### 1.1 Create the Framework\n",
    "\n",
    "Create a feature selection setup using `SequentialFeatureSelector`. The feature selection should use 3-fold cross-validation, classification accuracy as the scoring metric, and sequentially select the best-performing subset of features from the following: `\"age\"`, `\"credit_amount\"`, `\"credit_history\"`, and `\"duration\"`.\n",
    "\n",
    "We first provide code for data preprocessing as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique credit history values: ['critical/other existing credit', 'existing paid', 'delayed previously', 'no credits/all paid', 'all paid']\n",
      "Categories (5, object): ['all paid', 'critical/other existing credit', 'delayed previously', 'existing paid', 'no credits/all paid']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "# For simplification, select only the desired features\n",
    "selected_features = [\"age\", \"credit_amount\", \"credit_history\", \"duration\"]\n",
    "X_four_feats = X[selected_features].copy()\n",
    "\n",
    "categorical_features = [\"credit_history\", ]\n",
    "numerical_features = [feat for feat in selected_features if feat not in categorical_features]\n",
    "\n",
    "# This feature is ordinal, so we encode it using OrdinalEncoder\n",
    "unique_credit_history = X_four_feats['credit_history'].unique()\n",
    "print(f\"Unique credit history values: {unique_credit_history}\")\n",
    "\n",
    "# Define the categories based on actual values in the dataset\n",
    "credit_history_categories = [\n",
    "    'critical/other existing credit',  # Most risky - 0\n",
    "    'delayed previously',              # Somewhat risky - 1\n",
    "    'existing paid',                   # Moderate - 2\n",
    "    'no credits/all paid',             # Good - 3\n",
    "    'all paid'                         # Best - 4\n",
    "]\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder(\n",
    "    categories=[credit_history_categories],\n",
    "    dtype=np.int32\n",
    ")\n",
    "\n",
    "X_cat = X_four_feats[categorical_features]\n",
    "X_cat_encoded = pd.DataFrame(\n",
    "    ordinal_encoder.fit_transform(X_cat),\n",
    "    columns=categorical_features,\n",
    "    index=X_cat.index\n",
    ")\n",
    "\n",
    "X_num = X_four_feats[numerical_features]\n",
    "X_four_feats = pd.concat([X_num, X_cat_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now please write your code to solve the given task.\n",
    "\n",
    "<details><summary>Hint 1:</summary> \n",
    "Use `SequentialFeatureSelector()` from `sklearn.feature_selection` and set the parameter `cv=3`. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#===SOLUTION===\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(random_state=2025)\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=2025)\n",
    "\n",
    "sfs_forward = SequentialFeatureSelector(\n",
    "    clf, \n",
    "    n_features_to_select='auto',\n",
    "    direction='forward',\n",
    "    scoring='accuracy',\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Start the Feature Selection\n",
    "\n",
    "Start the feature selection by fitting the selector (`SequentialFeatureSelector`) to your reduced feature set (`X_four_feats`) and target (`y`). Then, identify the subset of selected features based on classification accuracy.\n",
    "\n",
    "<details><summary>Hint 1:</summary> \n",
    "Use the `.fit()` method of your initialized `SequentialFeatureSelector` object. \n",
    "</details> \n",
    "\n",
    "<details><summary>Hint 2:</summary> \n",
    "After fitting, use `.get_support()` or `.get_feature_names_out()` to identify which features have been selected. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['duration' 'credit_history']\n"
     ]
    }
   ],
   "source": [
    "#===SOLUTION===\n",
    "\n",
    "sfs_forward.fit(X_four_feats, y)\n",
    "print(f\"Selected features: {sfs_forward.get_feature_names_out()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fff3cd; border-left: 6px solid #ffa502; padding: 1em; margin: 1em 0; border-radius: 4px;\">\n",
    "  <strong>⚠️ Skipped Task: 1.3 Evaluate</strong>\n",
    "  <p>View the four characteristics and the accuracy from the instance archive for each of the first two batches.</p>\n",
    "  <p><em>This sub-section is skipped because the <code>SequentialFeatureSelector</code> does not store search results at intermediate steps, unlike the R solution.</em></p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Model Training\n",
    "\n",
    "Train the model with the subset of features indentified by the feature selector. Compute the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best feature set: ['duration' 'credit_history']\n",
      "Test accuracy with best features: 0.7000\n"
     ]
    }
   ],
   "source": [
    "#===SOLUTION===\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "best_feature_set = sfs_forward.get_feature_names_out()\n",
    "\n",
    "print(f\"Best feature set: {best_feature_set}\")\n",
    "\n",
    "X_best = sfs_forward.transform(X_four_feats)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_best, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Initialize and train the model with best features\n",
    "final_model = RandomForestClassifier(max_depth=10, random_state=2025)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "test_accuracy = final_model.score(X_test, y_test)\n",
    "print(f\"Test accuracy with best features: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Multiple Performance Measures\n",
    "\n",
    "To optimize multiple performance metrics, the procedure is similar to the previous step, but now multiple metrics are considered separately. Perform feature selection using random search to optimize three different metrics: classification accuracy, True Positive Rate (TPR), and True Negative Rate (TNR).\n",
    "\n",
    "Again, use the full `german_credit` dataset and perform the following steps to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's prepare the full dataset with proper encoding\n",
    "# Separate numerical and categorical features\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Apply ordinal encoding to categorical features\n",
    "X_cat_encoded = pd.DataFrame()\n",
    "for feature in categorical_features:\n",
    "    enc = OrdinalEncoder()\n",
    "    encoded_feature = enc.fit_transform(X[[feature]])\n",
    "    X_cat_encoded[feature] = encoded_feature.flatten()\n",
    "\n",
    "# Combine numerical and encoded categorical features\n",
    "X_num = X[numerical_features].copy()\n",
    "X_full = pd.concat([X_num, X_cat_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, please write the code to perform the following tasks:\n",
    "\n",
    "- Set up a random search feature selector.\n",
    "- Perform random search independently for each performance metric (`accuracy`, `TPR`, and `TNR`) using 3-fold cross-validation.\n",
    "- Evaluate and report the resulting feature subsets and their performances across all metrics.\n",
    "\n",
    "Use the provided `random_feature_search` function separately for each performance metric: `accuracy`, `tpr_scorer`, and `tnr_scorer`.\n",
    "\n",
    "<details><summary>Hint 1:</summary>\n",
    "After selecting the best features for each metric, evaluate the performance of each subset across all three metrics to compare results comprehensively.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def random_feature_search(X, y, clf, cv, scorer, n_iterations=50, min_features=1, max_features=None, rng = None):\n",
    "    \"\"\"This function is provided as a helper function to perform random search for feature selection.\"\"\"\n",
    "    rng = rng if rng is not None else np.random.RandomState(42)\n",
    "    \n",
    "    if max_features is None:\n",
    "        max_features = X.shape[1]\n",
    "    \n",
    "    all_features = list(X.columns)\n",
    "    best_score = -np.inf\n",
    "    best_features = []\n",
    "    \n",
    "    # Try different feature subset sizes\n",
    "    for _ in range(n_iterations):\n",
    "        # Randomly select number of features to use\n",
    "        n_features = rng.integers(min_features, max_features + 1)\n",
    "        \n",
    "        # Randomly select features without replacement\n",
    "        feature_indices = rng.choice(len(all_features), size=n_features, replace=False)\n",
    "        feature_subset = [all_features[i] for i in feature_indices]\n",
    "        \n",
    "        # Evaluate this feature subset\n",
    "        X_subset = X[feature_subset]\n",
    "        scores = cross_val_score(clf, X_subset, y, cv=cv, scoring=scorer)\n",
    "        avg_score = np.mean(scores)\n",
    "        \n",
    "        # Update best if this is better\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_features = feature_subset\n",
    "    \n",
    "    return best_features, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Multiple Performance Measures. Using custom random search for feature selection with multiple metrics\n",
      "Running random feature search...\n",
      "\n",
      "Feature selection results with different optimization criteria:\n",
      "         features_str  num_features  accuracy  classif.tpr  classif.tnr\n",
      "0  Accuracy optimized            18  0.752993     0.922851     0.356667\n",
      "1       TPR optimized             1  0.699999     1.000000     0.000000\n",
      "2       TNR optimized             9  0.731989     0.867148     0.416667\n",
      "\n",
      "Accuracy-optimized features: ['age', 'savings_status', 'property_magnitude', 'installment_commitment', 'personal_status', 'housing', 'credit_history', 'credit_amount', 'employment', 'other_payment_plans', 'checking_status', 'purpose', 'own_telephone', 'num_dependents', 'duration', 'job', 'foreign_worker', 'existing_credits']\n",
      "\n",
      "TPR-optimized features: ['own_telephone']\n",
      "\n",
      "TNR-optimized features: ['credit_amount', 'foreign_worker', 'purpose', 'checking_status', 'property_magnitude', 'other_parties', 'job', 'housing', 'duration']\n"
     ]
    }
   ],
   "source": [
    "#===SOLUTION===\n",
    "\n",
    "from sklearn.metrics import recall_score, make_scorer\n",
    "\n",
    "\n",
    "def tpr_score(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, pos_label=\"good\")\n",
    "\n",
    "def tnr_score(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, pos_label=\"bad\")\n",
    "\n",
    "tpr_scorer = make_scorer(tpr_score)\n",
    "tnr_scorer = make_scorer(tnr_score)\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=10, random_state=2025)\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=2025)\n",
    "\n",
    "print(\"2 Multiple Performance Measures. Using custom random search for feature selection with multiple metrics\")\n",
    "print(\"Running random feature search...\")\n",
    "\n",
    "# Function to evaluate feature set across all metrics\n",
    "def evaluate_feature_set(features, name):\n",
    "    X_subset = X_full[features]\n",
    "    acc = np.mean(cross_val_score(clf, X_subset, y, cv=cv, scoring='accuracy'))\n",
    "    tpr = np.mean(cross_val_score(clf, X_subset, y, cv=cv, scoring=tpr_scorer))\n",
    "    tnr = np.mean(cross_val_score(clf, X_subset, y, cv=cv, scoring=tnr_scorer))\n",
    "    \n",
    "    return {\n",
    "        \"features\": features,\n",
    "        \"features_str\": f\"{name} optimized\",\n",
    "        \"num_features\": len(features),\n",
    "        \"accuracy\": acc,\n",
    "        \"classif.tpr\": tpr,\n",
    "        \"classif.tnr\": tnr\n",
    "    }\n",
    "\n",
    "# Run random search for each metric\n",
    "metrics = {\n",
    "    'Accuracy': ('accuracy', None),\n",
    "    'TPR': (tpr_scorer, 'TPR'),\n",
    "    'TNR': (tnr_scorer, 'TNR')\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_features = {}\n",
    "\n",
    "# Perform feature selection for each metric\n",
    "for name, (scorer, _) in metrics.items():\n",
    "    features, score = random_feature_search(\n",
    "        X_full, y, clf, cv, scorer, n_iterations=20, rng=rng\n",
    "    )\n",
    "    best_features[name] = features\n",
    "    \n",
    "    # Evaluate this feature set across all metrics\n",
    "    results.append(evaluate_feature_set(features, name))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nFeature selection results with different optimization criteria:\")\n",
    "print(results_df[[\"features_str\", \"num_features\", \"accuracy\", \"classif.tpr\", \"classif.tnr\"]])\n",
    "\n",
    "print(\"\\nAccuracy-optimized features:\", best_features['Accuracy'])\n",
    "print(\"\\nTPR-optimized features:\", best_features['TPR'])\n",
    "print(\"\\nTNR-optimized features:\", best_features['TNR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: what is your observation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "===SOLUTION===\n",
    "\n",
    "Note that the measures can not be optimal at the same time so one has to choose according to their preferences. Here, we see different tradeoffs of sensitivity and specificity but no feature subset is dominated by another, i.e. has worse sensitivity and specificity than any other subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Nested Resampling\n",
    "\n",
    "Nested resampling enables finding unbiased performance estimators when feature selection is part of the modeling process. In Python with `scikit-learn`, this can be implemented by creating a custom wrapper class (e.g., `AutoFSelector`) that integrates feature selection within cross-validation.\n",
    "\n",
    "### 3.1 Create an AutoFSelector Instance\n",
    "\n",
    "Implement an `AutoFSelector` class that uses random search to find the subset of features resulting in the highest classification accuracy for a logistic regression model. The feature selector should perform the search using holdout validation and terminate after `n_evals` evaluations.\n",
    "\n",
    "Use the provided custom `AutoFSelector` class with logistic regression (`LogisticRegression`) as the base estimator.\n",
    "\n",
    "Note: the random search feature selection comes with large randomness. So, you may need to set `n_evals` to 20 and try multiple random seeds to get the best performance. In the solution, we use a lucky `random_state=543` for `AutoFSelector`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "\n",
    "\n",
    "class AutoFSelector(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_estimator, n_evals=10, cv=3, random_state=None):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_evals = n_evals\n",
    "        self.cv = cv\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Ensure X is a DataFrame to ease column handling\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            self.X_original_type = type(X)\n",
    "            if hasattr(X, 'columns'):\n",
    "                X_df = X.copy()\n",
    "            else:\n",
    "                # Create a DataFrame with default column names\n",
    "                X_df = pd.DataFrame(X)\n",
    "        else:\n",
    "            self.X_original_type = pd.DataFrame\n",
    "            X_df = X.copy()\n",
    "            \n",
    "        self.feature_names_ = X_df.columns.tolist()\n",
    "        \n",
    "        # Use the existing random_feature_search function\n",
    "        self.selected_features_, self.best_score_ = random_feature_search(\n",
    "            X_df, y, \n",
    "            self.base_estimator, \n",
    "            cv=StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=self.random_state),\n",
    "            scorer='accuracy', \n",
    "            n_iterations=self.n_evals,\n",
    "            min_features=1,\n",
    "            rng=np.random.default_rng(self.random_state)\n",
    "        )\n",
    "        \n",
    "        # Create the best mask for feature selection\n",
    "        self.best_mask_ = np.array([feat in self.selected_features_ for feat in self.feature_names_])\n",
    "        \n",
    "        # Fit the final model on the selected features\n",
    "        self.estimator_ = clone(self.base_estimator)\n",
    "        X_selected = X_df[self.selected_features_]\n",
    "        self.estimator_.fit(X_selected, y)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def _get_feature_subset(self, X):\n",
    "        \"\"\"Helper to get the selected feature subset from input X.\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # If X is already a DataFrame, simply select the features\n",
    "            return X[self.selected_features_]\n",
    "        else:\n",
    "            # If X is a numpy array, convert to DataFrame with proper column names\n",
    "            X_df = pd.DataFrame(X, columns=self.feature_names_)\n",
    "            return X_df[self.selected_features_]\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_subset = self._get_feature_subset(X)\n",
    "        return self.estimator_.predict(X_subset)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_subset = self._get_feature_subset(X)\n",
    "        return self.estimator_.predict_proba(X_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write your code for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#===SOLUTION===\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "log_reg = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=2025))\n",
    "])\n",
    "\n",
    "afs = AutoFSelector(\n",
    "    base_estimator=Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(random_state=2025))\n",
    "    ]),\n",
    "    n_evals=20,\n",
    "    cv=3,\n",
    "    random_state=543\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Benchmark\n",
    "\n",
    "Compare the `AutoFSelector` (nested resampling approach) against a plain logistic regression model using a 3-fold cross-validation on the Sonar dataset.\n",
    "\n",
    "<details><summary>Hint 1:</summary>\n",
    "Perform cross-validation separately for both the logistic regression and the `AutoFSelector`, then compute their average accuracies.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmark Results:\n",
      "           learner_id  classif.acc\n",
      "0       AutoFSelector     0.759834\n",
      "1  LogisticRegression     0.759696\n"
     ]
    }
   ],
   "source": [
    "# Load the Sonar dataset from OpenML\n",
    "X_sonar, y_sonar = fetch_openml('Sonar', version=1, as_frame=True, return_X_y=True)\n",
    "\n",
    "# Set up 3-fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=2025)\n",
    "\n",
    "# Compute CV accuracy for both models.\n",
    "score_afs = cross_val_score(afs, X_sonar, y_sonar, cv=cv, scoring='accuracy')\n",
    "score_lr = cross_val_score(log_reg, X_sonar, y_sonar, cv=cv, scoring='accuracy')\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"learner_id\": [\"AutoFSelector\", \"LogisticRegression\"],\n",
    "    \"classif.acc\": [score_afs.mean(), score_lr.mean()]\n",
    "})\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Wrapper methods calculate performance measures for various combinations of features in order to perform feature selection.\n",
    "* They are computationally expensive since several models need to be fitted.\n",
    "* The `AutoFSelector` inherits from the base classes `BaseEstimator` and `ClassifierMixin`, which is why it can be used like any other learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
