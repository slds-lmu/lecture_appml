{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf5188d-01b5-4fd1-86b5-e00278418c74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T10:19:10.146777Z",
     "start_time": "2025-03-26T10:19:08.914347Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Applied ML: Homework Exercise 02-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd340e03-9dee-4973-8b61-d5a445c92472",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Goal\n",
    "\n",
    "After this exercise, you should be able to navigate the building blocks of Bayesian optimization (BO) using `scikit-optimize` for general black box optimization problems, and more specifically, hyperparameter optimization (HPO).\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This section is a deep dive into Bayesian optimization (BO), also known as Model Based Optimization (MBO). BO is more complex than other tuning methods, so we will motivate theory and methodology first.\n",
    "\n",
    "## Black Box Optimization\n",
    "\n",
    "In hyperparameter optimization, learners are passed a hyperparameter configuration and evaluated on a given task via a resampling technique to estimate its generalization performance with the goal to find the optimal hyperparameter configuration. In general, this is a black box optimization problem, which considers the optimization of a function whose mathematical structure is unknown or unexploitable. The only thing we can observe is the generalization performance of the function given a hyperparameter configuration. As evaluating the performance of a learner can take a lot of time, HPO is an expensive black box optimization problem.\n",
    "\n",
    "## Bayesian Optimization\n",
    "\n",
    "There are many ways of doing black box optimization, grid and random search being examples of simple strategies. Bayesian optimization is a class of black box optimization algorithms that rely on a surrogate model trained on observed hyperparameter evaluations to model the black box function. This surrogate model tries to capture the unknown function between hyperparameter configurations and estimated generalization performance using (the very low number of) observed function evaluations. During each iteration, BO algorithms employ an acquisition function to determine the next candidate point for evaluation. This function measures the expected utility of each point within the search space based on the prediction of the surrogate model. The algorithm then selects the candidate point with the best acquisition function value and evaluates the black box function at that point to then update the surrogate model. This iterative process continues until a termination criterion is met, such as reaching a pre-specified maximum number of evaluations or achieving a desired level of performance. BO is a powerful method that often results in good optimization performance, especially if the cost of the black box evaluation becomes expensive and the optimization budget is tight.\n",
    "\n",
    "In the rest of this section, we will first provide an introduction to black box optimization with the scikit-optimize package and then introduce the building blocks of BO algorithms and examine their interplay and interaction during the optimization process before we assemble these building blocks in a ready-to-use black box optimizer with `scikit-optimize`.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Let’s load the packages required for this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b79c852f-dc93-4622-bfed-34170924ce05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:53:00.811923Z",
     "start_time": "2025-03-28T15:53:00.119259Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rng = np.random.default_rng(111)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce023bf-783a-42b4-9358-536e2433b5b0",
   "metadata": {},
   "source": [
    "Before we apply BO to hyperparameter optimization (HPO), we try to optimize the following simple sinusoidal function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca5591da-8ee6-4672-b05c-a17425984fce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:53:00.830336Z",
     "start_time": "2025-03-28T15:53:00.827781Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "# Define the sinusoidal function; expects a dict-like input with keys 'x1' and 'x2'\n",
    "def sinus_1D(xs: Dict[str, float]) -> float:\n",
    "    return 2 * xs['x1'] * np.sin(14 * xs['x1']) * np.sin(xs['x2']) * xs['x2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d053086-8223-4440-9ead-76d30a7229db",
   "metadata": {},
   "source": [
    "## 1 Building Blocks of BO\n",
    "\n",
    "Bayesian optimization (BO) usually follows this process:\n",
    "\n",
    "1. Generate and evaluate an initial design\n",
    "\n",
    "2. Loop:\n",
    "\n",
    "    2.1. Fit a surrogate model on the archive of all observations made so far to model the unknown black box function.\n",
    "\n",
    "    2.2. Optimize an acquisition function to determine which points of the search space are promising candidate(s) that should be evaluated next.\n",
    "\n",
    "    2.3. Evaluate the next candidate(s) and update the archive of all observations made so far.\n",
    "\n",
    "    2.4. Check if a given termination criterion is met; if not, go back to step 2.1.\n",
    "\n",
    "The acquisition function relies on the mean and standard deviation prediction of the surrogate model and requires no evaluation of the true black box function, making it comparably cheap to optimize. A good acquisition function will balance exploiting knowledge about regions where we observed that performance is good and the surrogate model has low uncertainty with exploring regions where it has not yet evaluated points and as a result the uncertainty of the surrogate model is high.\n",
    "\n",
    "BO is a highly modular algorithm: as long as the above structure is in place, then the surrogate models, acquisition functions, and acquisition function optimizers are all interchangeable to a certain extent. The design of `scikit-optimize` reflects this modularity, with key elements including surrogate models (e.g., Gaussian Process), acquisition functions (e.g., Expected Improvement), and optimizers for the acquisition functions. Let’s explore the interplay and interaction of these building blocks during optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232979ff-33af-4fb2-ac52-36718774bd9b",
   "metadata": {},
   "source": [
    "## 1.1 Initial design\n",
    "\n",
    "The initial set of points evaluated before a surrogate model can be fit is referred to as the initial design. `scikit-optimize` allows you to either construct this manually or use built-in methods to automate this step. We will demonstrate a manual method here.\n",
    "\n",
    "To construct an initial design, we will first try grid search, assuming an initial design of nine points on a domain of two numeric variables ranging from 0 to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf65a9fa-0a14-46ec-bfe1-da03b7d95918",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:53:01.068787Z",
     "start_time": "2025-03-28T15:53:01.064334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid design:\n",
      "    x1   x2\n",
      "0  0.0  0.0\n",
      "1  0.0  0.5\n",
      "2  0.0  1.0\n",
      "3  0.5  0.0\n",
      "4  0.5  0.5\n",
      "5  0.5  1.0\n",
      "6  1.0  0.0\n",
      "7  1.0  0.5\n",
      "8  1.0  1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a grid design for two variables ranging from 0 to 1 with resolution 3\n",
    "x_vals = np.linspace(0, 1, 3)\n",
    "# Use pd.DataFrame for the purpose of pretty print\n",
    "grid_design = pd.DataFrame([(x1, x2) for x1 in x_vals for x2 in x_vals], columns=['x1', 'x2'])\n",
    "print(f\"Grid design:\\n{grid_design}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816737f6-d651-4526-ba4f-d269291dcaf6",
   "metadata": {},
   "source": [
    "As you can see, this is essentially a DataFrame encoding the set of hyperparameter configurations we want to evaluate first, before any of the real BO magic starts.\n",
    "\n",
    "### Task\n",
    "\n",
    "Construct a more refined initial design, using SciPy to implement a Sobol design with 30 points, which has better coverage properties than grid or random search. If you are interested in why the Sobol design has favorable properties, you can take a look at the original paper by [Niederreiter (1988)](https://www.sciencedirect.com/science/article/pii/0022314X8890025X?via%3Dihub).\n",
    "\n",
    "<details><summary>Hint: </summary> \n",
    "    Wrap the code as a function as we will use it later. The signature should be `def get_sobol_design(n: int, d: int = 2) -> pd.DataFrame:` and the function uses [`scipy.stats.qmc.Sobol`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.qmc.Sobol.html) sequence to generate `n` points in a `d`-dimensional unit cube.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "284ee98bd93445cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:53:01.112273Z",
     "start_time": "2025-03-28T15:53:01.099143Z"
    },
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of Sobol design:\n",
      "         x1        x2\n",
      "0  0.833708  0.714247\n",
      "1  0.393502  0.301624\n",
      "2  0.166035  0.927012\n",
      "3  0.606692  0.088859\n",
      "4  0.641449  0.768583\n"
     ]
    }
   ],
   "source": [
    "#===SOLUTION===\n",
    "\n",
    "from scipy.stats import qmc\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Use SciPy's Sobol sequence to generate 30 points in a 2-dimensional unit cube\n",
    "def get_sobol_design(n: int, d: int = 2) -> pd.DataFrame:\n",
    "    sobol_sampler = qmc.Sobol(d=d, scramble=True)\n",
    "    # Suppress the warning: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", UserWarning)\n",
    "        sobol_design_array = sobol_sampler.random(n)\n",
    "    sobol_design = pd.DataFrame(sobol_design_array, columns=[f'x{i}' for i in range(1, d+1)])\n",
    "\n",
    "    return sobol_design\n",
    "\n",
    "sobol_design = get_sobol_design(30)\n",
    "print(f\"First 5 rows of Sobol design:\\n{sobol_design.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286862c7-836f-432f-b5f9-bea6fc71e177",
   "metadata": {},
   "source": [
    "## 1.2 Generate data from initial design\n",
    "\n",
    "Unlike R version which manually creates an `ObjectiveRFun` instance, in Python we simply apply the sinus_1D function to the Sobol design, obtaining the target. We store the target as an additional column to `sobol_design` DataFrame, for the purpose of pretty print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a94865c0171b4e75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:53:01.230422Z",
     "start_time": "2025-03-28T15:53:01.222242Z"
    },
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.970342</td>\n",
       "      <td>0.645288</td>\n",
       "      <td>0.641158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.160936</td>\n",
       "      <td>0.213977</td>\n",
       "      <td>0.011351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.360321</td>\n",
       "      <td>0.977642</td>\n",
       "      <td>-0.552262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.552327</td>\n",
       "      <td>0.412843</td>\n",
       "      <td>0.181627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.731361</td>\n",
       "      <td>0.817742</td>\n",
       "      <td>-0.634655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.415546</td>\n",
       "      <td>0.260273</td>\n",
       "      <td>-0.024989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.122744</td>\n",
       "      <td>0.555186</td>\n",
       "      <td>0.071058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.805533</td>\n",
       "      <td>0.116548</td>\n",
       "      <td>-0.020973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.862980</td>\n",
       "      <td>0.905423</td>\n",
       "      <td>-0.572770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.053580</td>\n",
       "      <td>0.454092</td>\n",
       "      <td>0.014551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2         y\n",
       "0  0.970342  0.645288  0.641158\n",
       "1  0.160936  0.213977  0.011351\n",
       "2  0.360321  0.977642 -0.552262\n",
       "3  0.552327  0.412843  0.181627\n",
       "4  0.731361  0.817742 -0.634655\n",
       "5  0.415546  0.260273 -0.024989\n",
       "6  0.122744  0.555186  0.071058\n",
       "7  0.805533  0.116548 -0.020973\n",
       "8  0.862980  0.905423 -0.572770\n",
       "9  0.053580  0.454092  0.014551"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#===SOLUTION===\n",
    "\n",
    "archive_data = get_sobol_design(30)\n",
    "archive_data['y'] = archive_data.apply(lambda row: sinus_1D(row), axis=1)\n",
    "\n",
    "archive_data.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb6caf0-cc4b-439a-a654-ec345b336d47",
   "metadata": {},
   "source": [
    "## 1.3 Train surrogate model\n",
    "\n",
    "A surrogate model wraps a regression learner that models the unknown black box function based on observed data. In `scikit-optimize`, surrogate models are typically implemented using regression learners such as Gaussian Processes.\n",
    "\n",
    "Here, we'll use a Gaussian Process regressor with a Matérn kernel. The Matérn covariance function is a kernel used in Gaussian processes to model the smoothness of the random function, offering a flexible class of smoothness parameters. The BFGS algorithm is a type of quasi-Newton method used for optimization, particularly effective in maximizing the likelihood in Gaussian process models by efficiently finding parameter estimates.\n",
    "\n",
    "Task: Update the surrogate model by fitting the Gaussian process and inspect the trained model's kernel parameters.\n",
    "\n",
    "<details><summary>Hint 1:</summary>\n",
    "    Use `GaussianProcessRegressor` and `sklearn.gaussian_process.kernels.Matern`.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3408b562d8cf2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:53:01.468393Z",
     "start_time": "2025-03-28T15:53:01.378306Z"
    },
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained surrogate model kernel: Matern(length_scale=1e-05, nu=2.5)\n"
     ]
    }
   ],
   "source": [
    "#===SOLUTION===\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning, module='sklearn')\n",
    "\n",
    "# Define a Gaussian Process regressor with a Matérn kernel (nu=2.5 corresponds to matern5_2)\n",
    "kernel = Matern(nu=2.5)\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=123)\n",
    "\n",
    "# Prepare training data: features are x1 and x2; target is y from the archive_data DataFrame\n",
    "X = archive_data[['x1', 'x2']]\n",
    "y = archive_data['y']\n",
    "\n",
    "# Update the surrogate model (i.e., fit the Gaussian process on the training data)\n",
    "gpr.fit(X, y)\n",
    "\n",
    "# Inspect the trained surrogate model's kernel (this shows the learned kernel parameters)\n",
    "print(f\"Trained surrogate model kernel: {gpr.kernel_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fff4770",
   "metadata": {},
   "source": [
    "### 1.4 Implementation of the Acquisition Function\n",
    "\n",
    "Implement the Expected Improvement (EI) acquisition function.\n",
    "\n",
    "The function should have the following docstring:\n",
    "\n",
    "```python\n",
    "def make_neg_ei(y_best: np.ndarray, gaussian_process: GaussianProcessRegressor) -> Callable[[ndarray], float]:\n",
    "    \"\"\"\n",
    "    Define a closure to generate a negative expected improvement function that captures the current y_best\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "<details><summary>Hint 1:</summary>\n",
    "    Use `skopt.aquisition.gaussian_ei` as the acquisition function.\n",
    "</details>\n",
    "\n",
    "<details><summary>Hint 2:</summary>\n",
    "    You need to define a negative expeteced improvement function that captures the current `y_best`. Why negative ei? Because `x_next = argmax_x EI(x)`, but the `scipy.optimize` solves argmin problems.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3afac08",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "#===SOLUTION===\n",
    "\n",
    "from skopt.acquisition import gaussian_ei \n",
    "\n",
    "\n",
    "def make_neg_ei(y_best, gaussian_process):\n",
    "    \"\"\"\n",
    "    Define a closure to generate a negative expected improvement function that captures the current y_best\n",
    "    Why negative ei? Because x_next = argmax_x EI(x), but the scipy.optimize solves argmin problems.\n",
    "    \"\"\"\n",
    "\n",
    "    def _neg_ei(x):\n",
    "        x = x.reshape(1, -1)\n",
    "        # Compute expected improvement using skopt's gaussian_ei; xi controls exploration-exploitation\n",
    "        ei = gaussian_ei(x, gaussian_process, y_opt=y_best, xi=0.01)\n",
    "        return -ei[0]\n",
    "    return _neg_ei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aa42e7-887f-4060-85f2-e41fe3081c50",
   "metadata": {},
   "source": [
    "## 2 Automating Bayesian Optimization\n",
    "Note that due to the discrepancy between mlr3 and scikit-optimize, we dont't need Section 1.5 in the R exercise.\n",
    "\n",
    "**Task**: Find the optimal hyperparameter configuration by iteratively updating the surrogate model and optimizing the acquisition function.\n",
    "\n",
    "For convenience, you can utilize the `skopt.gp_minimize` function, a readily available Bayesian Optimization tool that internally handles the steps outlined in Exercise 1. This allows you to specify Bayesian Optimization details directly through the arguments of `skopt.gp_minimize`, eliminating the need to manually define an acquisition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2ce9e29",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best y in initial instance archive: -0.9516148029280513\n",
      "Initial evaluations: 50\n",
      "\n",
      "Final incumbent candidate from automated BO:\n",
      "         x1        x2         y\n",
      "0  0.791804  0.999493 -1.326099\n"
     ]
    }
   ],
   "source": [
    "#===SOLUTION===\n",
    "\n",
    "from skopt import gp_minimize\n",
    "\n",
    "# Create wrapper to adapt sinus_1D to the format expected by skopt\n",
    "def objective(x):\n",
    "    return sinus_1D({'x1': x[0], 'x2': x[1]})\n",
    "\n",
    "# Define the search space\n",
    "dimensions = [(0.0, 1.0), (0.0, 1.0)]\n",
    "\n",
    "# Number of initial points and total evaluations\n",
    "n_initial_points = 50\n",
    "n_total_calls = 200\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    # ignore \"UserWarning: The balance properties of Sobol' points require n to be a power of 2\"\n",
    "    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "    # Run Bayesian optimization\n",
    "    result = gp_minimize(\n",
    "        func=objective,\n",
    "        dimensions=dimensions,\n",
    "        n_calls=n_total_calls - n_initial_points,\n",
    "        n_initial_points=n_initial_points,\n",
    "        initial_point_generator=\"sobol\",\n",
    "        acq_func=\"EI\",\n",
    "        random_state=111\n",
    "    )\n",
    "\n",
    "print(f\"Best y in initial instance archive: {min(result.func_vals[:n_initial_points])}\")\n",
    "print(f\"Initial evaluations: {n_initial_points}\")\n",
    "\n",
    "# Create dataframe with best result\n",
    "best_x = result.x\n",
    "best_y = result.fun\n",
    "best_candidate = pd.DataFrame({'x1': [best_x[0]], 'x2': [best_x[1]], 'y': [best_y]})\n",
    "\n",
    "print(\"\\nFinal incumbent candidate from automated BO:\")\n",
    "print(best_candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10db9989-9cbf-43d4-acc3-992221611684",
   "metadata": {},
   "source": [
    "## 3 BO for HPO with BayesSearch CV:\n",
    "\n",
    "In this task, your goal is to apply Bayesian hyperparameter optimization using `BayesSearchCV` from `scikit-optimize` to tune the hyperparameters of an SVM classifier with an RBF kernel on the sonar dataset.\n",
    "\n",
    "You should perform the following steps:\n",
    "\n",
    "1. Load and prepare the sonar dataset from OpenML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ab8f343-412e-4807-a8b4-60be9f24c5fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:53:04.095054Z",
     "start_time": "2025-03-28T15:53:04.054351Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml(\"sonar\", version=1, as_frame=True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e93f617-6417-4c15-ae47-6d126de5f62b",
   "metadata": {},
   "source": [
    "2. Define the SVM classifier and specify a hyperparameter search space for the C (cost) and gamma parameters, using log-uniform distributions.\n",
    "\n",
    "3. Define a scoring function to measure misclassification error.\n",
    "\n",
    "5. Configure and run Bayesian optimization using 3-fold stratified cross-validation.\n",
    "\n",
    "6. Identify and report the best hyperparameter configuration and its performance.\n",
    "\n",
    "The search space is:\n",
    "\n",
    "* `C`: `[1e-5, 1e5]`, log-scale.\n",
    "  \n",
    "* `gamma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "547451ee552941b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:53:10.026912Z",
     "start_time": "2025-03-28T15:53:04.114027Z"
    },
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best HP configuration (shown in original scale, not log-transformed)\n",
      "cost (C) = 3856.87501, gamma = 0.18238\n",
      "Misclassification error: 0.134714\n"
     ]
    }
   ],
   "source": [
    "#===SOLUTION===\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define the SVM classifier with a radial (RBF) kernel.\n",
    "# In scikit-learn, the SVM's cost parameter is 'C'.\n",
    "svm = SVC(kernel='rbf', probability=True)\n",
    "\n",
    "# Define the hyperparameter search space:\n",
    "# We use log-uniform distributions for 'C' (cost) and 'gamma'\n",
    "search_spaces = {\n",
    "    'C': (1e-5, 1e5, 'log-uniform'),\n",
    "    'gamma': (1e-5, 1e5, 'log-uniform')\n",
    "}\n",
    "\n",
    "# Define a scorer for misclassification error.\n",
    "def misclassification_error(y_true, y_pred):\n",
    "    return np.mean(y_true != y_pred)\n",
    "\n",
    "scorer = make_scorer(misclassification_error, greater_is_better=False)\n",
    "\n",
    "# Setup 3-fold stratified cross-validation\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)\n",
    "\n",
    "# Create the BayesSearchCV tuner.\n",
    "tuner = BayesSearchCV(\n",
    "    estimator=svm,\n",
    "    search_spaces=search_spaces,\n",
    "    n_iter=25,\n",
    "    scoring=scorer,\n",
    "    cv=cv,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "# Run the tuning procedure\n",
    "tuner.fit(X, y)\n",
    "\n",
    "# Extract the best hyperparameter configuration and performance.\n",
    "best_hp = tuner.best_params_\n",
    "# Note: the best_score_ is negative misclassification error.\n",
    "best_misclassification_error = -tuner.best_score_\n",
    "\n",
    "print(\"Best HP configuration (shown in original scale, not log-transformed)\")\n",
    "print(f\"cost (C) = {best_hp['C']:.5f}, gamma = {best_hp['gamma']:.5f}\")\n",
    "print(f\"Misclassification error: {best_misclassification_error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c058f",
   "metadata": {},
   "source": [
    "## 4 Bonus: Implementing BO Loop from Scratch\n",
    "\n",
    "In Exercise 2, you performed Bayesian Optimization using `skopt.gp_minimize`.\n",
    "\n",
    "Alternatively, you can implement the Bayesian Optimization Loop from scratch. The following hints may be helpful.\n",
    "\n",
    "<details><summary>Hint 1:</summary>\n",
    "    Use `scipy.optimize.differential_evolution` as the optimizer for the acquisition function.\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>Hint 2:</summary>\n",
    "The pseudo code is like this:\n",
    "    \n",
    "    1. Re-create the data (50 initial instances) as the initial archive data (which we will gradually expand).\n",
    "    \n",
    "    2. Define bounds corresponding to our domain.\n",
    "    \n",
    "    3. Define a closure to generate a negative expected improvement function.\n",
    "    \n",
    "    4. Set up how many instances we have, and how many instances (200) we are allowed to evaluate in total?\n",
    "\n",
    "    5. Then we enter the loop, while termination condition is not met:\n",
    "\n",
    "    6. Fit (update) the surrogate model using current archive data.\n",
    "\n",
    "    7. Compute the best observed objective value (for minimization, the smallest y).\n",
    "\n",
    "    8. Create a negative EI function based on the current best value.\n",
    "\n",
    "    9. Optimize the acquisition function using differential evolution.\n",
    "\n",
    "    10. Append the new candidate to the archive data.\n",
    "\n",
    "\n",
    "In the end, we print out the best sample in the archive data.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24080519-91a8-4ba2-bd46-c11e09e3176e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:53:04.048983Z",
     "start_time": "2025-03-28T15:53:01.529146Z"
    },
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best y in initial instance archive: -0.8605392053911493\n",
      "Initial evaluations: 50\n",
      "\n",
      "Final incumbent candidate from automated BO:\n",
      "          x1        x2         y\n",
      "75  0.837354  0.983681 -1.024403\n"
     ]
    }
   ],
   "source": [
    "#===SOLUTION===\n",
    "\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "# Re-create the data\n",
    "archive_data = get_sobol_design(50)\n",
    "archive_data['y'] = archive_data.apply(lambda row: sinus_1D(row), axis=1)\n",
    "print(f\"Best y in initial instance archive: {archive_data['y'].min()}\")\n",
    "\n",
    "# Define bounds corresponding to our domain\n",
    "bounds = [(0, 1), (0, 1)]\n",
    "\n",
    "n_evals = 200\n",
    "current_evals = len(archive_data)\n",
    "print(f\"Initial evaluations: {current_evals}\")\n",
    "# In a typical MBO loop, we would update the surrogate and add new evaluations until termination.\n",
    "while current_evals < n_evals:\n",
    "    # Fit (update) the surrogate model using current archive data\n",
    "    X = archive_data[['x1', 'x2']]\n",
    "    y = archive_data['y']\n",
    "    gpr.fit(X, y)\n",
    "    \n",
    "    # Compute the best observed objective value (for minimization, the smallest y)\n",
    "    y_best = y.min()\n",
    "    \n",
    "    # Create a negative EI function based on the current best value\n",
    "    neg_ei = make_neg_ei(y_best, gpr)\n",
    "    \n",
    "    # Optimize the acquisition function using differential evolution\n",
    "    result = differential_evolution(neg_ei, bounds, maxiter=300, seed=123)\n",
    "    candidate_x = result.x\n",
    "    candidate_y = sinus_1D({'x1': candidate_x[0], 'x2': candidate_x[1]})\n",
    "    \n",
    "    # Append the new candidate to the archive\n",
    "    new_row = {'x1': candidate_x[0], 'x2': candidate_x[1], 'y': candidate_y}\n",
    "    archive_data = pd.concat([archive_data, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    current_evals = len(archive_data)\n",
    "\n",
    "# At this point, the termination criterion is met.\n",
    "# Retrieve the best candidate from the instance (the incumbent)\n",
    "best_idx = archive_data['y'].idxmin()\n",
    "best_candidate = archive_data.loc[[best_idx], ['x1', 'x2','y']]\n",
    "\n",
    "print(\"\\nFinal incumbent candidate from automated BO:\")\n",
    "print(best_candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a60b0d9-dc40-4ccc-8743-5a172bd48d83",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this exercise, you learned how Bayesian Optimization can be effectively used in Python to solve general black-box optimization and hyperparameter optimization (HPO) problems specifically. Rather than evaluating arbitrary hyperparameter configurations, you optimized an acquisition function derived from a surrogate model (Gaussian Process) to iteratively propose new candidate configurations, efficiently identifying optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60284d75-0043-4087-b491-a9216517b9ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T15:53:10.079906Z",
     "start_time": "2025-03-28T15:53:10.075644Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
