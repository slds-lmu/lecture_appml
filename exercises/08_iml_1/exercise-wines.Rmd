---
name: "Wine"
title: "Wine quality"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, echo=FALSE}
set.seed(123)
longrun = FALSE
knitr::opts_chunk$set(cache = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = TRUE,
  R.options = list(width = 120))

colorize <- function(x, color) {
  # see https://bookdown.org/yihui/rmarkdown-cookbook/font-color.html
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}

show.solution = T
```

# Introduction

### Load the iml package

Before we start, we need to load some libraries:

* `randomForest` is a package that supplies the random forest algorithm for classification and regression. We use it to train a model for our data. 
* `iml` is a package that implements several model-agnostic interpretation methods that explain the the model behavior and its predictions.
* `ggplot`, `grid.Extra`, and `DataExplorer` will be used for plotting and a quick exploratory data analysis.

```{r library}
library('randomForest')
library('iml')
library('ggplot2')
library('gridExtra')
library('DataExplorer')
```

### Wine Data

```{r, out.width = "50%", echo=FALSE}
knitr::include_graphics("figure/wine-features.jpg")
```

We can import the wine data set from the 'data' folder and apply some pre-processing steps.

```{r data}
wine_complete = read.csv("wine.csv")
# Kicking out 36 wines with missing values
wine_complete = na.omit(wine_complete)
# convert wine type from data type character to type factor (create levels/categories for modelling purposes)
wine_complete$type = as.factor(wine_complete$type)
```

The data set:

* approx. 6500 red and white Portuguese "Vinho Verde" wines (the ratio between white and red is approximately 3:1)
* Features: Physicochemical properties
* Quality assessed by blind tasting, from 0 (very bad) to 10 (excellent)

```{r exploredta}
plot_bar(wine_complete)
plot_histogram(wine_complete)
```

### Model

Finally, we apply machine learning to predict the quality of wine using the random forest algorithm.

```{r, echo= FALSE, out.width=1000}
knitr::include_graphics("figure/random-forest.jpg")
```

Some interpretation methods make more sense if they are applied on test data (i.e., data that was not used to fit a model). 
Hence, we split up the data into a training set on which the random forest is trained and a test set which we will use to analyze our model using several interpretation methods.

We store the trained model in an object `rfmod` and use 1000 observations for the test set to speed up computational effort.

```{r model}
# sample 1000 observations randomly to speed things up
set.seed(1)
ind = sample(1:nrow(wine_complete), size = 1000, replace = FALSE)
wine_train = wine_complete[-ind,]
wine = wine_complete[ind,]

# now, we fit the random forest to be analyzed (this will take some time)
rfmod = randomForest(quality ~ ., data = wine_train)
rfmod
```

# Exercises

You can copy and paste the previously shown code to import the data and build the `rfmod` model.
Note that for all following exercises, you are asked to use the `wine` data that contains 1000 observations.

### Exercise 1: The Predictor 

The interpretation methods in the `iml` package require that the machine learning model and the data are wrapped in a Predictor object.
A Predictor object holds a machine learning model (mlr3, caret, randomForest, ...) and the data to be used for analyzing the model.

* Create a `Predictor` object using the `iml` package for the `wine` dataset and 
`rfmod` model. 
<details>
  <summary>**Hint**</summary>
Checkout the help page `?Predictor` to understand how to create a Predictor object.
</details>

* Make yourself familiar with the `Predictor` object. 
<details>
  <summary>**Hint**</summary> Also checkout the `data` field in the `Predictor`. </details>

* Can you find the `sample` method within the `Predictor` object?  
Sample 10 rows with replacement from the data. 

```{r, eval = !show.solution, echo = FALSE, results='asis'}
cat("<!--")
```

<details>
  <summary>**Solution**</summary>
  
Create a `Predictor` object. 
```{r predictor}
rfpred = Predictor$new(rfmod, data = wine, y = "quality")
rfpred
```

Sample 10 rows with replacement using the `sample` method
```{r sample}
rfpred$data$sample(n = 10, replace = TRUE)
```

</details>

```{r, eval = !show.solution, echo = FALSE, results='asis'}
cat("-->")
```

### Exercise 2: LIME

In the following, you are guided to implement LIME to interpret a Support Vector Machine (SVM). We use **two** (numeric) features and explore LIME on a multiclass classification problem with only two (numeric) features. The associated files for this exercise are [`lime.py`](https://raw.githubusercontent.com/slds-lmu/lecture_iml/master/exercises/06_lime_ce/code/lime.py) or [`lime.R`](https://raw.githubusercontent.com/slds-lmu/lecture_iml/master/exercises/06_lime_ce/code/lime.R) depending on your preferred programming language. 
In these files, helper functions for plotting (`get_grid()`, `plot_grid()` and `plot_points_in_grid()`) were already implemented.

#### a) Inspect Implemented Functions

First of all, make yourself familiar with the already implemented functions in the template files. 

* The function `get_grid()` prepares data to visualize the feature space. It creates a $N\times N$ grid, and every point in this grid is associated with a value. This value is obtained by the model's predict method.

* The function `plot_grid()`, visualizes the prediction surface. 

* The created plot is an input to the function `plot_points_in_grid()`, which adds given data points to the plot.

<details>
  <summary>**Solution**</summary>
  Example: 
```{r inspect}
set.seed(2022L)
library("e1071") # SVM 
library("gridExtra") # to plot two ggplots next to each other

dataset = read.csv(file = "rsrc/datasets/wheat_seeds.csv")
dataset$Type = as.factor(dataset$Type)
table(dataset$Type)

min_max_norm <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

dataset  = dataset[c("Perimeter", "Asymmetry.Coeff", "Type")]
dataset$Perimeter = min_max_norm(dataset$Perimeter)
dataset$Asymmetry.Coeff = min_max_norm(dataset$Asymmetry.Coeff)

traindata = dataset[sample(seq_len(nrow(dataset)),
                           round(0.6*nrow(dataset)), replace = TRUE),]

# Fit a svm to the data
mod = svm(Type ~ ., data = traindata)
dataset$Type = NULL

# Compute counterfactual for first observation
x_interest = data.frame(Perimeter = 0.31, Asymmetry.Coeff = 0.37)

# Parameters for method
points_per_feature = 50L
n_points = 1000L

# PREREQ -----------------------------------------------------------------------
source("rsrc/helper_functions_lime.R")

# plot -----------------------------------------------------------------------
grid = get_grid(model = mod, dataset = dataset, 
                points_per_feature = points_per_feature)

plot = plot_grid(grid)
plot
```
</details>

#### b) Sample Points

Your first implementation task is to sample points, which are later used to train the local surrogate model. Complete
`sample_points()` by randomly sampling from a uniform distribution. Consider the lower and
upper bounds from the input data points. 

<details>
  <summary>**Hint**</summary> In Python, you can use the method `dataset.get_configspace().get_hyperparameters_dict()` 
implemented in the file [utils/dataset.py](https://raw.githubusercontent.com/slds-lmu/lecture_iml/master/exercises/06_lime_ce/code/utils/dataset.py) to retrieve the lower and upper values. 
For an example, have a look on the already implemented function `get_grid()`.
</details>

#### c) Weight Points

Given a selected point $\mathbf{x}$ and the sampled points $Z$ from the previous task, we now want to weight the points. Use the following equation with $d$ as Euclidean distance to calculate the weight of a single point $\mathbf{z} \in Z$:

\begin{equation}
    \phi_{\mathbf{x}}(\mathbf{z}) = exp(-d(\mathbf{x}, \mathbf{z})^2/\sigma^2).
    \label{eq:dist}
\end{equation}

To make plotting easier later on, the weights should be normalized between zero and one. Finally, return the normalized weights in `weight_points()`.

#### d) Fit Local Surrogate Model

Finally, fit a decision tree with training data and weights. Return the fitted tree in the function `fit_explainer_model()`. 
What could be problematic?

### Exercise 3: Counterfactuals

This exercise provides the fundamental ideas of another local IML approach which is called *Counterfactual Explanations*. 
A counterfactual explanation of a prediction describes the smallest change to the feature values that changes the prediction to a predefined output.
We can manually change some feature values and inspect how the prediction changes.

Your task is to choose a wine with a predicted rating of 5 and to change some of its feature values so that the predicted wine rating changes to 6. 
Try to change as few features as possible (e.g., 1-2 features) and with rather small changes.

<details>
  <summary>**Hint**</summary>  After having chosen a wine with quality 5, you need to manually change feature values of 1-2 features of this wine until its predicted wine quality is higher than 6 (while holding all other features unchanged). To do so, you may create a sequence of feature values using the `seq` function and may use the `expand.grid` function to obtain a grid containing all possible combinations of multiple sequences, e.g.,
```{r}
# create sequences of feature 1 and feature 2
feat1 = seq(1, 2, length.out = 3)
feat2 = seq(10, 12, length.out = 3)
# use expand.grid to have all possible combinations of the sequences
expand.grid(feat1, feat2)
```
</details>

```{r, eval = !show.solution, echo = FALSE, results='asis'}
cat("<!--")
```


<details>
  <summary>**Solution**</summary>
We know that, generally speaking, volatile acidity has a rather negative effect on the predicted quality while alcohol has a rather positive effect.
Therefore, we'll search for a wine with low alcohol (<9.5) but high volatile acidity (>0.5) and then, artificially, turn those values up, respectively down, until the rating for the regarded observation reaches a value of 6.

1) Search for a suitable wine
```{r incr_wineq}
preds = rfpred$predict(wine)
idx = which(preds >=4.9 & preds <= 5.1 & wine$alcohol < 9.5 & wine$volatile.acidity > 0.5)
wine1 = wine[idx[1], ]
wine1$quality = NULL # remove target variable
wine1
```

2) Create a grid with a sequence and turn up alcohol volume slowly to 12 and volatile acidity down to 0.2
```{r incr_wineq1}
alc_steps = seq(from = wine1$alcohol, to = 13, length.out = 30)
va_steps = seq(from = wine1$volatile.acidity, to = 0.2, length.out = 30)
grid_wine = expand.grid(alcohol = alc_steps, volatile.acidity = va_steps)
dim(grid_wine)
head(grid_wine)
```

3) Create a new dataframe - keep all features constant (values of the regarded wine) and only vary alcohol and acidity (by using the created grid)
```{r incr_wineq2}
cols = setdiff(colnames(wine1), colnames(grid_wine))
cols # these are the columns of features that are kept oonstant
wine_experiment = cbind(wine1[, cols], grid_wine)
head(wine_experiment)
```

4) Predict with the rfmod model to see how the predictions for the regarded wine changes when we only vary the alcohol and acidity features while keeping all other features constant.
```{r incr_wineq3}
rfmod_pred = predict(rfmod, wine_experiment)
wine_experiment$rfmod_prediction = rfmod_pred
rmarkdown::paged_table(wine_experiment[wine_experiment$rfmod_prediction > 6, 
  c("alcohol", "volatile.acidity", "rfmod_prediction")])
```

For the considered wine, we reach the prediction of wine quality > 6 at around 12%-13% alcohol and volatile acidity of around 0.35. 

```{r last_plot}
p1 = ggplot(wine_experiment, aes(alcohol, volatile.acidity)) +
  geom_raster(aes(fill = rfmod_prediction)) +
  scale_fill_viridis_c()

p2 = ggplot(wine_experiment, aes(alcohol, volatile.acidity)) +
  geom_point(aes(col = factor(rfmod_prediction > 6)))

library(patchwork)
p1 / p2
```

</details>
```{r, eval = !show.solution, echo = FALSE, results='asis'}
cat("-->")
```



### Exercise 4: ICE

### Exercise 5: Partial Dependence Plot (PDP)


Besides knowing which features were important, we are interested in how the features influence the predicted wine quality. 
To address this aspect, you are asked to plot the PDP (average marginal effect curve) of the feature alcohol and to interpret the result.

 
<details>
  <summary>**Hint**</summary> Use `FeatureEffect$new` and specify `method="pdp"` for the partial dependence method.
</details>

```{r, eval = !show.solution, echo = FALSE, results='asis'}
cat("<!--")
```


<details>
  <summary>**Solution**</summary>
  
The `FeatureEffect` class implements accumulated local effect (ALE) plots (which is the default `method`), partial dependence plots (which needs to be specified via `method="pdp"`) and individual conditional expectation curves (which need to be specified via `method="pdp+ice"`).

The following plot shows the partial dependence (PD) for the feature alcohol.
The marks (rug plot) on the x-axis indicate the distribution of the feature alcohol, showing how relevant a region is for interpretation (little or no points mean that predictions might be uncertain in this area and hence we cannot reliably interpret these regions).

```{r gec}
alc_pdp = FeatureEffect$new(rfpred, feature = "alcohol", method = "pdp")
alc_pdp$plot()
```

The estimated average marginal effect seems to increase monotonically with increasing values of alcohol. 
This suggests that, on average, the predicted quality of wine increases with the volume of alcohol.
However, at an alcohol level of around 12, the wine quality does not seem to improve much.
</details>
```{r, eval = !show.solution, echo = FALSE, results='asis'}
cat("-->")
```


### Exercise 6: PDP and ICE

Plot the PDP and ICE curves of the feature alcohol. What do you observe?
Can you also draw conclusions about whether alcohol interacts with other features? 
 
<details>
  <summary>**Hint**</summary> `FeatureEffect$new` with `method="pdp+ice"`
</details>

```{r, eval = !show.solution, echo = FALSE, results='asis'}
cat("<!--")
```

<details>
  <summary>**Solution**</summary>
Individual conditional expectation (ICE) plots visualize how the model prediction of individual observations for one feature changes by varying its feature values while keeping all other features' values fixed.

```{r pdpice}
alc_pdp_ice = FeatureEffect$new(rfpred, feature = "alcohol", method = "pdp+ice")
alc_plot_ice = alc_pdp_ice$plot()
alc_plot_ice
```

Since there are ICE curves whose behavior differs from the majority of ICE curves
(especially for low alcohol levels)
we could assume that alcohol interacts with other features. 

</details>
```{r, eval = !show.solution, echo = FALSE, results='asis'}
cat("-->")
```


### Exercise 7: Plot the PDP (global effects) of many Features 

Until now, we only looked at the marginal effect of the feature alcohol. 
Now you are asked to visualize and interpret the marginal effect of the **six most important** features.
 
<details>
  <summary>**Hint**</summary> You get the names of the six most important features by inspecting the first 6 rows in 
`results` of the `FeatureImp` object from Exercise 2.  
</details>
 
<details>
  <summary>**Hint**</summary> Use `FeatureEffects$new` with `features` set to the
names of the six most important variables. The calculation might take a bit. 
To speed things up, you could reduce the number of grid points (the default is 20) on the x-axis of the PDP via the argument `grid.size` (but don't use a too small value).
</details>
 
<details>
  <summary>**Hint**</summary>  You may need to install the package `patchwork`. If you haven't installed the package yet, install the package to your machine with `install.packages("patchwork")`.
  </details>

```{r, eval = !show.solution, echo = FALSE, results='asis'}
cat("<!--")
```


<details>
  <summary>**Solution**</summary>
  We extract the names of the six most important features from the `results` of the `FeatureImp`
  object in Exercise 2. 
```{r most_imp}
most_imp = f_imp$results$feature[1:6]
```

```{r f_effects}
f_effects = FeatureEffects$new(rfpred, features = most_imp, method = "pdp", grid.size = 20)
f_effects$plot()
```

We can observe that the higher the importance score is, the steeper is the marginal effect curve.

Especially the distributions of `free.sulfur.dioxide` and `sulphates` are 
right-skewed, such that their PD values for high feature values need to be regarded with caution. 


</details>
```{r, eval = !show.solution, echo = FALSE, results='asis'}
cat("-->")
```



### Exercise 8: 2D-PDP

Compute a 2D-PDP between alcohol and `volatile.acidity`.

<details>
  <summary>**Hint**</summary>  In previous exercises, we used `FeatureEffect$new` to create 1-dimensional PDPs. But now you are asked to create a 2-dimensional PDP for alcohol and acidity. Look up the help page `?FeatureEffect` to find out how to do this.
  </details>

```{r, eval = !show.solution, echo = FALSE, results='asis'}
cat("<!--")
```


<details>
  <summary>**Solution**</summary>

```{r, alc_acidity_pdp, eval=longrun}
alc_pdp = FeatureEffect$new(rfpred, feature = c("alcohol", "volatile.acidity"), method = "pdp", grid.size = 10)
alc_pdp$plot()
```
```{r save2D, echo=FALSE, eval=longrun}
ggsave("figure/2D.png", plot = alc_pdp$plot(), width = 6, height = 4)
```
```{r load2D, echo=FALSE, eval=!longrun}
knitr::include_graphics("figure/2D.png")
```

The 2D-PDP visualizes the interaction between alcohol and acidity. The higher the alcohol volume and the less acid a wine is the higher is its quality. However, it needs to be noted that there are just a few observations for very high values of acidity and hence predictions in this region need to be regarded with caution.

</details>
```{r, eval = !show.solution, echo = FALSE, results='asis'}
cat("-->")
```


### Exercise 9: ALE

### Exercise 10: Feature Importance

Find out which features are important using the mean absolute error (mae):

* Create a plot that shows the feature importance of all features and interpret the result. 
<details>
  <summary>**Hint**</summary> Use `FeatureImp` and the mean absolute error ('mae') as loss function. If needed, checkout the help page via `?FeatureImp`. 
</details>

* If you used the implemented `plot` function for visualization you should 
see intervals surrounding the importance scores. What do they mean? 

* Find a way to access the raw values of the feature importance results table. 
<details>
  <summary>**Hint**</summary> You need to inspect the R-object that is created with `FeatureImp`. It contains the importance scores in a data.frame (see `?FeatureImp`).
</details>

```{r, eval = !show.solution, echo = FALSE, results='asis'}
cat("<!--")
```


<details>
  <summary>**Solution**</summary>
We can measure the loss-based importance for each feature with `FeatureImp`. The resulting importance scores are calculated by shuffling each feature individually and measuring how much the performance drops (or in this case how much the 'mae' (loss) increases) compared to the original model's performance. For this regression task, we choose to measure the loss with the mean absolute error ('mae'), another choice would be the mean squared error ('mse').

Once we create a new object of `FeatureImp`, the importance is automatically computed.
We can call the `plot()` function of the object ...

```{r fimportance}
f_imp = FeatureImp$new(rfpred, loss = "mae")
plot(f_imp)
```

... or look at the results in a data.frame.

```{r fimportance_result}
results = f_imp$results
rmarkdown::paged_table(results) # this line produces a nice table output in the html
```

We receive 5 % and 95 % quantiles of the importance values because
per default, the shuffling is repeated 5 times (via `n.repetitions` in `FeatureImp`)
for more stable and accurate results.
The intervals in the plot present the range between these two quantiles. 

In this example, `r results[1,"feature"]` and `r results[2,"feature"]` seem to have the highest contribution to the prediction of wine quality among all features. 

The scores refer to the ratio between the mae values after permuting the considered feature vs. without permuting the considered feature (see the description of the `compare` argument in `?FeatureImp`). The higher these values, the more important a feature.

</details>
```{r, eval = !show.solution, echo = FALSE, results='asis'}
cat("-->")
```


### Exercise 11: LOCO

### Exercise 12: Further hypotheses

Think about further interesting hypotheses that you can answer by applying the introduced IML methods.

<details>
  <summary>**Hint**</summary> Some suggestions: 

  - Is there a predicted difference in wine quality between red and white wines? 
This can be analyzed by ICE plots and PDPs.
  - What are the feature attributions for the data point with the lowest predicted
quality? This can be analyzed by Shapley values. 
</details>

