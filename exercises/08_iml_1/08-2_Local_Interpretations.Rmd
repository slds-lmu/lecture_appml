---
title: "Counterfactuals - WhatIf"
subtitle: "In-class exercise 08-2"
output:
  prettydoc::html_pretty:
    theme: hpstr
    toc: yes
    toc_depth: 2
    css: ../_template/prettydoc-hpstr.css
    self_contained: yes
---

```{r, include=FALSE}
sys.source("setup.R", envir = knitr::knit_global())
```

# Goal

Deepening your understanding of counterfactuals by inspecting another counterfactuals algorithm.

# Prerequisites

We need the following libraries:

```{r library}
library(randomForest)
library(docstring)
library(StatMatch)
```

# 1 WhatIf Counterfactuals

Counterfactual explanations are a valuable tool to explain predictions of 
machine learning models. They tell the user how features need to be changed 
in order to predict a desired outcome. 
One of the simplest approaches to generate counterfactuals is to determine for 
a given observation `x` (`x_interest`) the closest data point which has a prediction equal to 
the desired outcome.^[Wexler et al. (2019): "The What-If Tool: Interactive Probing of Machine Learning Models"]

In the following, an implementation of this so called *WhatIf* approach for a binary classifier is given:

```{r}
generate_whatif = function(x_interest, model, dataset) {
  #' Computes whatif counterfactuals for binary classification models, 
  #' i.e., the closest data point with a different prediction.
  #
  #' @param x_interest (data.frame): Datapoint of interest, a single row data set. 
  #' @param model: Binary classifier which can call a predict method.
  #' @param dataset (data.frame): Input data
  #'
  #' @return counterfactual (data.frame): data.frame with one row presenting the counterfactuals
  #'    closest to  `x_interest` with a different prediction.

  # subset dataset to the observations having a prediction different to x_interest
  pred = predict(model, newdata = x_interest)
  preddata = predict(model, dataset)
  idx = which(preddata != pred)
  dataset = dataset[idx,]
  
  # Pairwise Gower distances 
  dists = StatMatch::gower.dist(data.x = x_interest, data.y = dataset)
  minid = order(dists)[1]
  
  # Return nearest datapoint
  return(dataset[minid,]) 
}
```

In this exercise we consider the wheat seeds dataset (`wheat_seeds.csv`). 
It consists of seven features to distinct three types of wheat kernels (0-Canadian, 1-Kama, 2-Rosa).
Converge the target `Type` into a binary classification problem to predict whether the wheat kernel belongs to the type *Rosa*.
Fit a random forest on the data set and compute a WhatIf-counterfactual for the first observation.


<details>
  <summary>**Hint 1:**</summary>
  
The binary target should be converted into a factor variable. 
With this, the values receive new meanings: 1-notRosa, 2-Rosa.

</details>

```{r, eval = !show.solution, echo = FALSE, results='asis'}
cat("<!--")
```

### Solution:

<details>
<summary>Click me:</summary>

```{r, eval = show.solution}
df = read.csv(file = "../data/wheat_seeds.csv")
table(df$Type)

# Create a binary classification task
# all observations of Type Canadian are set to Type = 1
# so Type = 1 now contains obs of Type != Rosa
df$Type = as.factor(ifelse(df$Type == "0", 1, df$Type))
table(df$Type)

# Fit a random forest to the data
mod = randomForest::randomForest(Type ~ ., data = df)

# Compute counterfactual for first observation
x_interest = df[1,]
x_interest
```

```{r, eval = show.solution}
cf = generate_whatif(x_interest = x_interest, model = mod, dataset = df)
cf
```

```{r, eval = !show.solution, echo = FALSE, results='asis'}
cat("-->")
```

# 2 Analysis of attributes

Which attributes from the lecture (*validity*, *sparsity*, ...) does this approach fulfill. Based on this, derive the advantages and disadvantages of the approach.

```{r, eval = !show.solution, echo = FALSE, results='asis'}
cat("<!--")
```

### Solution:

<details>
<summary>Click me:</summary>

Counterfactuals generated with WhatIf are valid and proximal, since they reflect the closest training datapoint 
with the desired/different prediction. 
The counterfactuals are also plausible since by definition they adhere to the data manifold.
The counterfactuals are not sparse and might propose changes to many features - this is 
a clear disadvantage of this method.

```{r, eval = !show.solution, echo = FALSE, results='asis'}
cat("-->")
```

# 3 Evaluation

In order to evaluate the sparseness of the counterfactual produced by WhatIf, we could use the following approach: 
For each feature of the counterfactual instance assess whether setting its 
value to the one of `x_interest` still leads to a different prediction than `x_interest`.

Create a function `evaluate_counterfactual()` using the following steps:

* Create an empty vector `feature_nams`.
* For each feature do the following: 
    1. Create a copy of the counterfactual.
    2. Replace the feature value of this copy with the value of `x_interest`. 
    3. Evaluate if the prediction for this copy still differs to the one of `x_interest`. 
    4. If it still differs, add the name of this feature to `feature_nams`.
* End for - return `feature_nams`.

Evaluate the counterfactual derived in the previous exercise.

<details>
  <summary>**Hint 1:**</summary>
  
```{r, eval = FALSE}
evaluate_counterfactual = function(counterfactual, x_interest, model) {
  #' Evaluates if counterfactuals are minimal, i.e., if setting one feature to 
  #' the value of x_interest still results in a different prediction than for x_interest.
  #' 
  #' @param counterfactual (data.frame): Counterfactual of `x_interest`, a single row data set. 
  #' @param x_interest (data.frame): Datapoint of interest, a single row data set. 
  #' @param model: Binary classifier which can call a predict method.
  #'
  #' @return (list): List with names of features that if set for the counterfactual to the value of 
  #' `x_interest`, still leads to a different prediction than for x_interest. 
 
  ...
}
```

</details>

```{r, eval = !show.solution, echo = FALSE, results='asis'}
cat("<!--")
```

### Solution:

<details>
<summary>Click me:</summary>

```{r, eval = show.solution}
evaluate_counterfactual = function(counterfactual, x_interest, model) {
  #' Evaluates if counterfactuals are minimal, i.e., if setting one feature to 
  #' the value of x_interest still results in a different prediction than for x_interest.
  #' 
  #' @param counterfactual (data.frame): Counterfactual of `x_interest`, a single row data set. 
  #' @param x_interest (data.frame): Datapoint of interest, a single row data set. 
  #' @param model: Binary classifier which can call a predict method.
  #'
  #' @return (list): List with names of features that if set for the counterfactual to the value of 
  #' `x_interest`, still leads to a different prediction than for x_interest. 
  pred = predict(model, newdata = x_interest)
  feature_nams = c()
  for (feature in names(counterfactual)) {
    if (counterfactual[feature] == x_interest[feature]) {
      next
    }
    newcf = counterfactual
    newcf[, feature] = x_interest[, feature]
    newpred = predict(model, newcf)
    if (newpred != pred) {
      feature_nams = c(feature_nams, feature)
    }
  }
  return(feature_nams)
}
```

```{r, eval = show.solution}
df$Type = NULL
x_interest$Type = NULL
cf$Type = NULL
evaluate_counterfactual(counterfactual = cf, x_interest = x_interest, model = mod)
```

```{r, eval = !show.solution, echo = FALSE, results='asis'}
cat("-->")
```


# Summary

We discovered and became familiar with the WhatIf algorithm and its advantages and disadvantages.
